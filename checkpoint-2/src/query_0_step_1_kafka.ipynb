{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07aae79b-5afd-4d6b-856d-ab51d8a11dc4",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d5a3f-ac3a-4f0b-a346-bde9a7b09e58",
   "metadata": {},
   "source": [
    "Before proceeding we need to ensure kafka has the topic we will use.\n",
    "`raw-taxi-data`.\n",
    "For the partitions, we are using the `medallion` as the partition key ensuring the related records go to the same partition.\n",
    "\n",
    "We will produce the data using the csv as the datasource and then we read the stream from kafka, load into dataframe and start doing the EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12bc7a13-ef08-408c-a6e6-bd694560e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, to_json, struct, lit, current_timestamp, expr\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108591fc-8201-40fb-ac87-538fb1ec2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Remove the empty space in each cell of the header, schema parsing was erroring.\n",
    "# If erroring run it from the shell.\n",
    "# !for file in input/trip_data_*.csv; do sed -i '1s/, /,/g' \"$file\"; done\n",
    "# !for file in input/sample.csv; do sed -i '1s/, /,/g' \"$file\"; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14f3568-ff4e-4749-84ed-a572ab89c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample_df = pd.read_csv('input/sample.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63833eb-1813-4cb2-8655-47885114dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['medallion', 'hack_license', 'vendor_id', 'rate_code',\n",
       "       'store_and_fwd_flag', 'pickup_datetime', 'dropoff_datetime',\n",
       "       'passenger_count', 'trip_time_in_secs', 'trip_distance',\n",
       "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
       "       'dropoff_latitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be262cbe-8a4d-40ba-b927-9a9d5edca3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m builder \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNYC Taxi Data Kafka ETL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.session.timeZone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.extensions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.spark_catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure_spark_with_delta_pip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# noise reduction\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder.appName(\"NYC Taxi Data Kafka ETL\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# noise reduction\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3796d6c6-f412-42c2-810f-d4a3fee1a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_taxi_schema = StructType(\n",
    "    [\n",
    "        StructField(\"medallion\", StringType(), True),\n",
    "        StructField(\"hack_license\", StringType(), True),\n",
    "        StructField(\"vendor_id\", StringType(), True),\n",
    "        StructField(\"rate_code\", StringType(), True),\n",
    "        StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"passenger_count\", IntegerType(), True),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d612cb4a-7731-4406-bc47-44c0d7541a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_kafka(csv_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Read a CSV file and publish records to Kafka using Spark's partitioning\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Ingesting data from {csv_path} to Kafka topic 'raw-taxi-data'\"\n",
    "    )\n",
    "    \n",
    "    # Load the data\n",
    "    taxi_data = spark.read.csv(csv_path, header=True, schema=raw_taxi_schema)\n",
    "    \n",
    "    # Get total count for reporting\n",
    "    total_count = taxi_data.count()\n",
    "    print(f\"Total records to process: {total_count}\")\n",
    "    \n",
    "    # Add partition key\n",
    "    taxi_data = taxi_data.withColumn(\"kafka_key\", col(\"medallion\"))\n",
    "    \n",
    "    # Create the JSON structure\n",
    "    kafka_batch = taxi_data.select(\n",
    "        col(\"kafka_key\").cast(\"string\"),\n",
    "        to_json(\n",
    "            struct(*[col(c) for c in taxi_data.columns if c != \"kafka_key\"])\n",
    "        ).alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    # Write to Kafka in one go (Spark will handle the batching internally)\n",
    "    kafka_batch.write.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"raw-taxi-data\") \\\n",
    "        .option(\"maxOffsetsPerTrigger\", batch_size) \\\n",
    "        .save()\n",
    "    \n",
    "    print(f\"Finished ingesting data to Kafka topic 'raw-taxi-data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc48395-c9bc-48c3-aad2-99848e7268dc",
   "metadata": {},
   "source": [
    "## Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45f330-5e35-44bc-97f6-d552056d7067",
   "metadata": {},
   "source": [
    "We are doing simulation of producing events and then sending them to kafka. Next step is to subscribe to topic and read the events as a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f055f326-7abc-45fb-99b7-6348bdfbe9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data from input/trip_data_8.csv to Kafka topic 'raw-taxi-data'\n",
      "Total records to process: 12597109\n",
      "Finished ingesting data to Kafka topic 'raw-taxi-data'\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"input/trip_data_8.csv\"\n",
    "ingest_csv_to_kafka(csv_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bafbe3-1409-4c68-a507-1319c897368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop() # Double check if we might need to improve run-time by restarting, since this is a \"data pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5049d-9735-4722-b165-a85710cc4baa",
   "metadata": {},
   "source": [
    "## Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314ab29c-6459-415c-ac1b-5fb4090d1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"raw-taxi-data\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216d7b18-6f66-4306-9358-ed9cab4f6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_kafka_stream = raw_stream.select(\n",
    "    col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "    col(\"value\").cast(\"string\").alias(\"json_data\"),\n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf90945a-0907-4514-b601-a94017ccf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_taxi_schema\n",
    "\n",
    "from pyspark.sql.functions import from_json # extract data from json\n",
    "\n",
    "raw_data_df = filtered_kafka_stream.select(\n",
    "    \"kafka_key\",\n",
    "    from_json(\"json_data\", raw_taxi_schema).alias(\"data\"),\n",
    "    \"kafka_timestamp\"\n",
    ").select(\"kafka_key\", \"kafka_timestamp\", \"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af23ccaf-f16a-402b-a263-d8892f73ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kafka_key: string (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a9ebdf-96cf-47b2-84b9-8de6b4495938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['kafka_key, 'kafka_timestamp, data.*]\n",
      "+- Project [kafka_key#21, from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(vendor_id,StringType,true), StructField(rate_code,StringType,true), StructField(store_and_fwd_flag,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(passenger_count,IntegerType,true), StructField(trip_time_in_secs,IntegerType,true), StructField(trip_distance,DoubleType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), json_data#22, Some(UTC)) AS data#27, kafka_timestamp#23]\n",
      "   +- Project [cast(key#7 as string) AS kafka_key#21, cast(value#8 as string) AS json_data#22, timestamp#12 AS kafka_timestamp#23]\n",
      "      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7b697fac, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@43a32e5e, [startingOffsets=earliest, kafka.bootstrap.servers=kafka:9092, subscribe=raw-taxi-data], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6f96be8d,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka:9092, subscribe -> raw-taxi-data, startingOffsets -> earliest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "kafka_key: string, kafka_timestamp: timestamp, medallion: string, hack_license: string, vendor_id: string, rate_code: string, store_and_fwd_flag: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, trip_time_in_secs: int, trip_distance: double, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double\n",
      "Project [kafka_key#21, kafka_timestamp#23, data#27.medallion AS medallion#61, data#27.hack_license AS hack_license#62, data#27.vendor_id AS vendor_id#63, data#27.rate_code AS rate_code#64, data#27.store_and_fwd_flag AS store_and_fwd_flag#65, data#27.pickup_datetime AS pickup_datetime#66, data#27.dropoff_datetime AS dropoff_datetime#67, data#27.passenger_count AS passenger_count#68, data#27.trip_time_in_secs AS trip_time_in_secs#69, data#27.trip_distance AS trip_distance#70, data#27.pickup_longitude AS pickup_longitude#71, data#27.pickup_latitude AS pickup_latitude#72, data#27.dropoff_longitude AS dropoff_longitude#73, data#27.dropoff_latitude AS dropoff_latitude#74]\n",
      "+- Project [kafka_key#21, from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(vendor_id,StringType,true), StructField(rate_code,StringType,true), StructField(store_and_fwd_flag,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(passenger_count,IntegerType,true), StructField(trip_time_in_secs,IntegerType,true), StructField(trip_distance,DoubleType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), json_data#22, Some(UTC)) AS data#27, kafka_timestamp#23]\n",
      "   +- Project [cast(key#7 as string) AS kafka_key#21, cast(value#8 as string) AS json_data#22, timestamp#12 AS kafka_timestamp#23]\n",
      "      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7b697fac, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@43a32e5e, [startingOffsets=earliest, kafka.bootstrap.servers=kafka:9092, subscribe=raw-taxi-data], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6f96be8d,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka:9092, subscribe -> raw-taxi-data, startingOffsets -> earliest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [kafka_key#21, kafka_timestamp#23, data#27.medallion AS medallion#61, data#27.hack_license AS hack_license#62, data#27.vendor_id AS vendor_id#63, data#27.rate_code AS rate_code#64, data#27.store_and_fwd_flag AS store_and_fwd_flag#65, data#27.pickup_datetime AS pickup_datetime#66, data#27.dropoff_datetime AS dropoff_datetime#67, data#27.passenger_count AS passenger_count#68, data#27.trip_time_in_secs AS trip_time_in_secs#69, data#27.trip_distance AS trip_distance#70, data#27.pickup_longitude AS pickup_longitude#71, data#27.pickup_latitude AS pickup_latitude#72, data#27.dropoff_longitude AS dropoff_longitude#73, data#27.dropoff_latitude AS dropoff_latitude#74]\n",
      "+- Project [cast(key#7 as string) AS kafka_key#21, from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(vendor_id,StringType,true), StructField(rate_code,StringType,true), StructField(store_and_fwd_flag,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(passenger_count,IntegerType,true), StructField(trip_time_in_secs,IntegerType,true), StructField(trip_distance,DoubleType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), cast(value#8 as string), Some(UTC)) AS data#27, timestamp#12 AS kafka_timestamp#23]\n",
      "   +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7b697fac, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@43a32e5e, [startingOffsets=earliest, kafka.bootstrap.servers=kafka:9092, subscribe=raw-taxi-data], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@6f96be8d,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka:9092, subscribe -> raw-taxi-data, startingOffsets -> earliest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [kafka_key#21, kafka_timestamp#23, data#27.medallion AS medallion#61, data#27.hack_license AS hack_license#62, data#27.vendor_id AS vendor_id#63, data#27.rate_code AS rate_code#64, data#27.store_and_fwd_flag AS store_and_fwd_flag#65, data#27.pickup_datetime AS pickup_datetime#66, data#27.dropoff_datetime AS dropoff_datetime#67, data#27.passenger_count AS passenger_count#68, data#27.trip_time_in_secs AS trip_time_in_secs#69, data#27.trip_distance AS trip_distance#70, data#27.pickup_longitude AS pickup_longitude#71, data#27.pickup_latitude AS pickup_latitude#72, data#27.dropoff_longitude AS dropoff_longitude#73, data#27.dropoff_latitude AS dropoff_latitude#74]\n",
      "+- Project [cast(key#7 as string) AS kafka_key#21, from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(vendor_id,StringType,true), StructField(rate_code,StringType,true), StructField(store_and_fwd_flag,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(passenger_count,IntegerType,true), StructField(trip_time_in_secs,IntegerType,true), StructField(trip_distance,DoubleType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), cast(value#8 as string), Some(UTC)) AS data#27, timestamp#12 AS kafka_timestamp#23]\n",
      "   +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ba0ec3-0e18-4e8e-ae6a-827f98dbb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking stream retrieval of data\n",
    "# raw_stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0120d8-60dd-40e6-bf91-487dbe4d98ab",
   "metadata": {},
   "source": [
    "```bash\n",
    "big_data_project_pyspark  | Batch: 0\n",
    "big_data_project_pyspark  | -------------------------------------------\n",
    "big_data_project_pyspark  | +----+--------------------+\n",
    "big_data_project_pyspark  | | key|               value|\n",
    "big_data_project_pyspark  | +----+--------------------+\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"341...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"6D3...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"6D4...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"4C4...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"125...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"3B0...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"1A5...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"A30...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"EF7...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"C64...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"820...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"820...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"C4C...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"6B8...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"EB2...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"EB2...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"6D4...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"AA5...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"733...|\n",
    "big_data_project_pyspark  | |NULL|{\"medallion\":\"733...|\n",
    "big_data_project_pyspark  | +----+--------------------+\n",
    "big_data_project_pyspark  | only showing top 20 rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "908419a2-b93d-44e0-a377-f5fb60c005e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query name: raw_taxi_query\n",
      "Status: {'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Is active: True\n"
     ]
    }
   ],
   "source": [
    "for query in spark.streams.active:\n",
    "    print(f\"Query name: {query.name}\")\n",
    "    print(f\"Status: {query.status}\")\n",
    "    print(f\"Is active: {query.isActive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd472a-90d6-40e4-83ce-6b6d9dcf7edf",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778c726c-afca-4a6d-baae-c2b937a4d8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/raw_taxi_data/_delta_log'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "table_name = \"raw_taxi_data\"\n",
    "warehouse_dir = spark.conf.get('spark.sql.warehouse.dir').replace('file:', '')\n",
    "output_path = os.path.join(warehouse_dir, table_name)\n",
    "checkpoint_path = \"streaming/taxis/_checkpoint\" \n",
    "\n",
    "# output_path = f\"{table_name}\" # XXX: check this issue later. we already point the store of delta tables to the spark-warehouse dir in the config of spark\n",
    "\n",
    "raw_query = (raw_data_df.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"delta\")\n",
    "  .queryName(\"raw_taxi_query\")\n",
    "  .trigger(processingTime=\"10 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")\n",
    "\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(60):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "create_table_if_exists(output_path,table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bc6d159-9186-4c3b-9a5f-d6d6b09c0c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "Is active: True\n",
      "Recent progress: []\n"
     ]
    }
   ],
   "source": [
    "print(raw_query.status)\n",
    "print(f\"Is active: {raw_query.isActive}\")\n",
    "print(f\"Recent progress: {raw_query.recentProgress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75341884-1b4e-46c9-8521-322629e1eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0e3291a-bea0-4408-82d6-6bd3665b49c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|raw_taxi_data|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3418082-8661-44a1-99bb-f23a9e58e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|           kafka_key|              string|   NULL|\n",
      "|     kafka_timestamp|           timestamp|   NULL|\n",
      "|           medallion|              string|   NULL|\n",
      "|        hack_license|              string|   NULL|\n",
      "|           vendor_id|              string|   NULL|\n",
      "|           rate_code|              string|   NULL|\n",
      "|  store_and_fwd_flag|              string|   NULL|\n",
      "|     pickup_datetime|           timestamp|   NULL|\n",
      "|    dropoff_datetime|           timestamp|   NULL|\n",
      "|     passenger_count|                 int|   NULL|\n",
      "|   trip_time_in_secs|                 int|   NULL|\n",
      "|       trip_distance|              double|   NULL|\n",
      "|    pickup_longitude|              double|   NULL|\n",
      "|     pickup_latitude|              double|   NULL|\n",
      "|   dropoff_longitude|              double|   NULL|\n",
      "|    dropoff_latitude|              double|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|spark_catalog.def...|       |\n",
      "|                Type|            EXTERNAL|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED raw_taxi_data;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d75ab-2764-4aa8-81c5-71f25b3542e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.listdir(output_path))\n",
    "#print(\"Delta log:\", os.listdir(f\"{output_path}/_delta_log\") if os.path.exists(f\"{output_path}/_delta_log\") else \"No delta log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d62edb29-64a6-4e3f-a83c-e339b4338537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta log entries: 5\n",
      "First few log entries:\n",
      "{'commitInfo': {'timestamp': 1743097340555, 'operation': 'STREAMING UPDATE', 'operationParameters': {'outputMode': 'Append', 'queryId': '0bf902da-0d45-4099-8b1b-8121a9bb7f49', 'epochId': '0'}, 'isolationLevel': 'Serializable', 'isBlindAppend': True, 'operationMetrics': {'numRemovedFiles': '0', 'numOutputRows': '25194218', 'numOutputBytes': '1085501949', 'numAddedFiles': '1'}, 'engineInfo': 'Apache-Spark/3.5.3 Delta-Lake/3.3.0', 'txnId': 'a3c503d0-e07a-4342-9b1b-ffa3d5ce1724'}}\n",
      "{'metaData': {'id': 'ebc28110-31a2-4892-a9bc-14be5daced42', 'format': {'provider': 'parquet', 'options': {}}, 'schemaString': '{\"type\":\"struct\",\"fields\":[{\"name\":\"kafka_key\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"kafka_timestamp\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"medallion\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"hack_license\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"vendor_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"rate_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"store_and_fwd_flag\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"pickup_datetime\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dropoff_datetime\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"passenger_count\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trip_time_in_secs\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"trip_distance\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"pickup_longitude\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"pickup_latitude\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dropoff_longitude\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dropoff_latitude\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}', 'partitionColumns': [], 'configuration': {}, 'createdTime': 1743097129440}}\n",
      "{'protocol': {'minReaderVersion': 1, 'minWriterVersion': 2}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f\"{output_path}/_delta_log/00000000000000000000.json\", \"r\") as f:\n",
    "    delta_log = [json.loads(line) for line in f]\n",
    "print(\"Delta log entries:\", len(delta_log))\n",
    "print(\"First few log entries:\")\n",
    "for entry in delta_log[:3]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaff6f4-7834-45d6-aff1-743a75802312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging parquet file\n",
    "#parquet_file = \"spark-warehouse/raw_taxi_data/part-00000-f0351f15-e91c-457a-8015-e1150ef04ff4-c000.snappy.parquet\"\n",
    "#parquet_df = spark.read.parquet(parquet_file)\n",
    "#print(f\"Number of records in parquet file: {parquet_df.count()}\")\n",
    "#display(parquet_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5236cf-24ae-4d36-ae22-5a1bf556e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(parquet_df.describe().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1eec99-f524-4f11-9020-9146fe18084f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a79e69a3-e1b2-4b95-8235-cf305cd20689",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_batch = spark.read.format(\"delta\").table(\"default.raw_taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f09bf25-35f4-4b2e-ac30-787faeab6abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'UnresolvedRelation [default, raw_taxi_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "kafka_key: string, kafka_timestamp: timestamp, medallion: string, hack_license: string, vendor_id: string, rate_code: string, store_and_fwd_flag: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, trip_time_in_secs: int, trip_distance: double, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double\n",
      "SubqueryAlias spark_catalog.default.raw_taxi_data\n",
      "+- Relation spark_catalog.default.raw_taxi_data[kafka_key#2964,kafka_timestamp#2965,medallion#2966,hack_license#2967,vendor_id#2968,rate_code#2969,store_and_fwd_flag#2970,pickup_datetime#2971,dropoff_datetime#2972,passenger_count#2973,trip_time_in_secs#2974,trip_distance#2975,pickup_longitude#2976,pickup_latitude#2977,dropoff_longitude#2978,dropoff_latitude#2979] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation spark_catalog.default.raw_taxi_data[kafka_key#2964,kafka_timestamp#2965,medallion#2966,hack_license#2967,vendor_id#2968,rate_code#2969,store_and_fwd_flag#2970,pickup_datetime#2971,dropoff_datetime#2972,passenger_count#2973,trip_time_in_secs#2974,trip_distance#2975,pickup_longitude#2976,pickup_latitude#2977,dropoff_longitude#2978,dropoff_latitude#2979] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.default.raw_taxi_data[kafka_key#2964,kafka_timestamp#2965,medallion#2966,hack_license#2967,vendor_id#2968,rate_code#2969,store_and_fwd_flag#2970,pickup_datetime#2971,dropoff_datetime#2972,passenger_count#2973,trip_time_in_secs#2974,trip_distance#2975,pickup_longitude#2976,pickup_latitude#2977,dropoff_longitude#2978,dropoff_latitude#2979] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/home/jovyan/spark-warehouse/raw_taxi_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<kafka_key:string,kafka_timestamp:timestamp,medallion:string,hack_license:string,vendor_id:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df_batch.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64edf1-c415-487f-a358-1c7c7d16858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_stats = delta_df.describe()\n",
    "\n",
    "# stats\n",
    "# stats = desc_stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad9877-9fca-4cbf-a7e3-3ccc19cbe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebdd3e-297a-4b51-af06-045ab5d458b5",
   "metadata": {},
   "source": [
    "## Data Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62dc3a4d-726f-4c40-8013-c5b728ba9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|raw_taxi_data|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ffc1a90-c9e0-42bb-8339-88ee92663cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-------+\n",
      "|          col_name|data_type|comment|\n",
      "+------------------+---------+-------+\n",
      "|         kafka_key|   string|   NULL|\n",
      "|   kafka_timestamp|timestamp|   NULL|\n",
      "|         medallion|   string|   NULL|\n",
      "|      hack_license|   string|   NULL|\n",
      "|         vendor_id|   string|   NULL|\n",
      "|         rate_code|   string|   NULL|\n",
      "|store_and_fwd_flag|   string|   NULL|\n",
      "|   pickup_datetime|timestamp|   NULL|\n",
      "|  dropoff_datetime|timestamp|   NULL|\n",
      "|   passenger_count|      int|   NULL|\n",
      "| trip_time_in_secs|      int|   NULL|\n",
      "|     trip_distance|   double|   NULL|\n",
      "|  pickup_longitude|   double|   NULL|\n",
      "|   pickup_latitude|   double|   NULL|\n",
      "| dropoff_longitude|   double|   NULL|\n",
      "|  dropoff_latitude|   double|   NULL|\n",
      "+------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE TABLE raw_taxi_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef3b97b2-9b1f-4cde-b871-2f3bc5eed7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark warehouse dir: file:/home/jovyan/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark warehouse dir: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e862170-78b1-42c8-acf1-ed1ea528d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|kafka_key|     kafka_timestamp|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|    pickup_datetime|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|     NULL|2025-03-27 17:12:...|3418135604CD3F357...|B25386A1F259C8744...|      VTS|        1|              NULL|2013-08-30 07:57:00|2013-08-30 08:30:00|              5|             1980|        14.58|      -73.791359|      40.645657|       -73.922501|       40.758766|\n",
      "|     NULL|2025-03-27 17:12:...|B341CD110148050E6...|F73AA91687B1C2E23...|      VTS|        1|              NULL|2013-08-25 16:26:00|2013-08-25 16:36:00|              1|              600|         1.24|      -73.986221|      40.759483|       -73.995247|       40.749817|\n",
      "|     NULL|2025-03-27 17:12:...|CD6A088A792463164...|C3A87F0847E12D914...|      CMT|        1|                 N|2013-08-02 08:01:40|2013-08-02 08:05:10|              1|              209|          0.3|      -73.982117|      40.749187|        -73.97892|       40.752605|\n",
      "|     NULL|2025-03-27 17:12:...|4F34ACD42D64890E1...|58A7E1A66282EA656...|      VTS|        1|              NULL|2013-08-18 16:15:00|2013-08-18 16:35:00|              1|             1200|         3.22|      -73.971603|      40.765316|        -73.98819|       40.757427|\n",
      "|     NULL|2025-03-27 17:12:...|09D95C058609B6396...|CE3675661661E1289...|      VTS|        1|              NULL|2013-08-25 16:32:00|2013-08-25 16:35:00|              1|              180|         0.45|      -73.952667|      40.778446|       -73.957184|       40.778542|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {table_name} LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "069902c9-dc98-4285-b480-b6f097aad16b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for streaming query to process data...\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnull, isnan, count, lit, expr\n",
    "\n",
    "# cleaned_data paths\n",
    "clean_table_name = \"clean_taxi_data\"\n",
    "\n",
    "checkpoint_path = \"streaming/taxis/_checkpoint\" \n",
    "clean_checkpoint_path = \"streaming/taxis/_checkpoint_clean\" \n",
    "clean_output_path = os.path.join(warehouse_dir, clean_table_name)\n",
    "\n",
    "# Define cleaning logic to apply to the stream\n",
    "def clean_taxi_data(df):\n",
    "    return df \\\n",
    "        .filter(col(\"medallion\").isNotNull()) \\\n",
    "        .filter(col(\"trip_distance\") > 0) \\\n",
    "        .filter(col(\"passenger_count\") > 0) \\\n",
    "        .filter(col(\"trip_time_in_secs\") > 0) \\\n",
    "        .filter(\n",
    "            (col(\"pickup_longitude\") != 0) & \n",
    "            (col(\"pickup_latitude\") != 0) & \n",
    "            (col(\"dropoff_longitude\") != 0) & \n",
    "            (col(\"dropoff_latitude\") != 0)\n",
    "        ) \\\n",
    "        .filter(\n",
    "            (col(\"pickup_longitude\") >= -180) & \n",
    "            (col(\"pickup_longitude\") <= 180) & \n",
    "            (col(\"pickup_latitude\") >= -90) & \n",
    "            (col(\"pickup_latitude\") <= 90) &\n",
    "            (col(\"dropoff_longitude\") >= -180) & \n",
    "            (col(\"dropoff_longitude\") <= 180) &\n",
    "            (col(\"dropoff_latitude\") >= -90) & \n",
    "            (col(\"dropoff_latitude\") <= 90)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"trip_speed_mph\", \n",
    "            when(col(\"trip_time_in_secs\") > 0, \n",
    "                 (col(\"trip_distance\") / (col(\"trip_time_in_secs\") / 3600))\n",
    "            ).otherwise(0)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"is_valid_trip\", \n",
    "            (col(\"trip_distance\") > 0) & \n",
    "            (col(\"trip_time_in_secs\") > 0) &\n",
    "            (col(\"trip_speed_mph\") < 100)  # Filter unrealistic speeds\n",
    "        )\n",
    "\n",
    "# Set up the streaming query from the raw table to the clean table\n",
    "raw_df = spark.readStream.format(\"delta\").load(output_path)\n",
    "\n",
    "# Apply the cleaning transformations\n",
    "clean_df = clean_taxi_data(raw_df)\n",
    "\n",
    "# Start the streaming query to the clean table\n",
    "clean_query = (clean_df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .queryName(\"clean_taxi_query\")\n",
    "    .trigger(processingTime=\"10 second\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"checkpointLocation\", clean_checkpoint_path)\n",
    "    .start(clean_output_path)\n",
    ")\n",
    "\n",
    "# Wait for some data to be processed\n",
    "import time\n",
    "print(\"Waiting for streaming query to process data...\")\n",
    "time.sleep(30)  # Wait 30 seconds to allow some data to be processed\n",
    "\n",
    "# Check if clean data exists and create table\n",
    "create_table_if_exists(clean_output_path, clean_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73fc8393-e056-473f-b454-5b961955dd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n",
      "[Errno 2] No such file or directory: '/home/jovyan/spark-warehouse/clean_taxi_data/_delta_log'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if clean data exists and create table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcreate_table_if_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_table_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 25\u001b[0m, in \u001b[0;36mcreate_table_if_exists\u001b[0;34m(output_path, table_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m         files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(output_path)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _f \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if clean data exists and create table\n",
    "create_table_if_exists(clean_output_path, clean_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64f8b4cc-b977-42bd-9184-fe3e07ed0f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/formatters.py:347\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    345\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:617\u001b[0m, in \u001b[0;36mSparkSession._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124m        <div>\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[38;5;124m</b></p>\u001b[39m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m--> 617\u001b[0m         catalogImplementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalogImplementation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    618\u001b[0m         sc_HTML\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39m_repr_html_(),\n\u001b[1;32m    619\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff4151dfd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41aa3b45-8e3c-4acd-b1d2-0f43c50b0447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclean_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs active: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_query\u001b[38;5;241m.\u001b[39misActive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecent progress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_query\u001b[38;5;241m.\u001b[39mrecentProgress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:250\u001b[0m, in \u001b[0;36mStreamingQuery.status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstatus\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    Returns the current status of the query.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    >>> sq.stop()\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "print(clean_query.status)\n",
    "print(f\"Is active: {clean_query.isActive}\")\n",
    "print(f\"Recent progress: {clean_query.recentProgress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adacb7e-fd9c-4af9-adea-dd04947a6a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
