{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53c7125-ec7b-4ed8-85c5-971d627cf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, window, unix_timestamp, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, to_json, struct, lit, current_timestamp, expr, when, from_json, window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create schema fields\n",
    "schema_fields = [\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"delay\", DoubleType(), True)\n",
    "]\n",
    "\n",
    "# Add fields for the 10 route positions\n",
    "for i in range(1, 11):\n",
    "    schema_fields.append(StructField(f\"start_cell_id_{i}\", StringType(), True))\n",
    "    schema_fields.append(StructField(f\"end_cell_id_{i}\", StringType(), True))\n",
    "\n",
    "# Create schema\n",
    "output_schema = StructType(schema_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958eb352-02de-49e3-a0c6-9ade0c95d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a consistent warehouse directory path - use absolute path\n",
    "WAREHOUSE_DIR = \"./spark-warehouse\"\n",
    "\n",
    "# Create the schema definition for NYC taxi data\n",
    "def create_raw_taxi_schema():\n",
    "    \"\"\"\n",
    "    Create the complete schema for NYC taxi data based on the DEBS Grand Challenge 2015 specification\n",
    "    \n",
    "    This schema includes all fields from the original dataset:\n",
    "    - Basic identifiers (medallion, hack_license)\n",
    "    - Time data (pickup_datetime, dropoff_datetime, trip_time_in_secs)\n",
    "    - Trip information (trip_distance)\n",
    "    - Location coordinates (pickup/dropoff longitude/latitude)\n",
    "    - Payment information (payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        # Taxi and driver identifiers\n",
    "        StructField(\"medallion\", StringType(), True),         # Taxi vehicle identifier (md5sum)\n",
    "        StructField(\"hack_license\", StringType(), True),      # Taxi license identifier (md5sum)\n",
    "        \n",
    "        # Trip time information\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),   # Time of passenger pickup\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),  # Time of passenger dropoff\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),   # Duration of the trip in seconds\n",
    "        \n",
    "        # Trip distance\n",
    "        StructField(\"trip_distance\", DoubleType(), True),     # Trip distance in miles\n",
    "        \n",
    "        # Pickup coordinates\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),  # Longitude coordinate of pickup\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),   # Latitude coordinate of pickup\n",
    "        \n",
    "        # Dropoff coordinates\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True), # Longitude coordinate of dropoff\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),  # Latitude coordinate of dropoff\n",
    "        \n",
    "        # Payment information\n",
    "        StructField(\"payment_type\", StringType(), True),      # Payment method (credit card or cash)\n",
    "        StructField(\"fare_amount\", DoubleType(), True),       # Fare amount in dollars\n",
    "        StructField(\"surcharge\", DoubleType(), True),         # Surcharge in dollars\n",
    "        StructField(\"mta_tax\", DoubleType(), True),           # Tax in dollars\n",
    "        StructField(\"tip_amount\", DoubleType(), True),        # Tip in dollars\n",
    "        StructField(\"tolls_amount\", DoubleType(), True),      # Bridge and tunnel tolls in dollars\n",
    "        \n",
    "        # Additional fields that may be present\n",
    "        StructField(\"total_amount\", DoubleType(), True)       # Total amount paid (calculated field)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36920d78-6e27-49ef-8d85-4426360499e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "def create_spark_session(app_name=\"FrequentRoutes\"):\n",
    "    \"\"\"\n",
    "    start spark session with kafka and delta support / memory config setup too\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder.appName(app_name) \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.network.timeout\", \"800s\") \\\n",
    "        .config(\"spark.sql.broadcastTimeout\", \"1000s\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", WAREHOUSE_DIR) \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.driver.memory\", \"48g\") \\\n",
    "        .config(\"spark.executor.memory\", \"36g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"22g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"22g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.default.parallelism\", \"200\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "        .enableHiveSupport()\n",
    "    \n",
    "    # delta config\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # do not flood logs\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    # Print configs for debugging\n",
    "    print(f\"Warehouse directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "    print(f\"Catalog implementation: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bba791-f537-4334-89a3-213ac5f0e98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/gpfs/helios/home/andressebastian1/dev/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /gpfs/helios/home/andressebastian1/.ivy2/cache\n",
      "The jars for the packages stored in: /gpfs/helios/home/andressebastian1/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f423c86e-d41c-452d-9869-17d5e1230b3a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 128ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f423c86e-d41c-452d-9869-17d5e1230b3a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "25/03/30 20:50:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse directory: file:/gpfs/helios/home/andressebastian1/big-data-management/spark-warehouse\n",
      "Catalog implementation: hive\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb54654-704e-42a8-b0d1-42ac68525461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define paths\n",
    "CLEAN_TABLE_NAME = \"clean_taxi_data\"\n",
    "FREQ_TABLE_NAME = \"freq_taxi_data\"\n",
    "FREQ_TABLE_NAME_2 = \"freq_taxi_data_part_2\"\n",
    "\n",
    "CLEAN_OUTPUT_PATH = os.path.join(WAREHOUSE_DIR, CLEAN_TABLE_NAME)\n",
    "FREQ_OUTPUT_PATH = os.path.join(WAREHOUSE_DIR, FREQ_TABLE_NAME)\n",
    "FREQ_OUTPUT_PATH_2 = os.path.join(WAREHOUSE_DIR, FREQ_TABLE_NAME_2)\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(WAREHOUSE_DIR, \"streaming/checkpoints\")\n",
    "\n",
    "CLEAN_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"clean_taxi_data\")\n",
    "FREQ_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"frequent_routes\")\n",
    "FREQ_ROUTES_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"frequent_routes\")\n",
    "\n",
    "SOURCE_DELTA_TABLE = CLEAN_OUTPUT_PATH\n",
    "OUTPUT_DELTA_TABLE = FREQ_OUTPUT_PATH_2\n",
    "CHECKPOINT_LOCATION = FREQ_ROUTES_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"frequent_routes_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85dc2b0-8216-4d58-bd47-631791d30436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "CELL_SIZE = 500  # size of grid cell in meters\n",
    "ORIGIN_LAT = 41.474937  # Latitude of cell 1.1\n",
    "ORIGIN_LON = -74.913585  # Longitude of cell 1.1\n",
    "MAX_CELL = 300  # grid expands 150km with 500m cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529fa1c1-f653-436a-b5f6-1ec19337972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import math\n",
    "\n",
    "def query1_part2_streaming():\n",
    "    \"\"\"\n",
    "    Streaming implementation for Query 1 Part 2 - Detect and output changes \n",
    "    in the top 10 most frequent routes during the last 30 minutes.\n",
    "    \n",
    "    Output format: pickup_datetime, dropoff_datetime, start_cell_id_1, end_cell_id_1, \n",
    "                  ..., start_cell_id_10, end_cell_id_10, delay\n",
    "    \"\"\"\n",
    "    \n",
    "    # Register UDF for creating cell ID in format \"x.y\"\n",
    "    @F.udf(returnType=StringType())\n",
    "    def get_cell_id(x, y):\n",
    "        if x is None or y is None:\n",
    "            return None\n",
    "        return f\"{x}.{y}\"\n",
    "    \n",
    "    # Define UDFs for converting coordinates to grid cells\n",
    "    @F.udf(returnType=IntegerType())\n",
    "    def lat_to_cell_y(lat):\n",
    "        # Convert latitude to cell Y coordinate (south)\n",
    "        if lat is None:\n",
    "            return None\n",
    "        meters_south = (ORIGIN_LAT - lat) * 111000  # Approx conversion to meters\n",
    "        cell_y = int(meters_south / CELL_SIZE) + 1\n",
    "        return cell_y if 1 <= cell_y <= MAX_CELL else None\n",
    "\n",
    "    @F.udf(returnType=IntegerType())\n",
    "    def lon_to_cell_x(lon, lat):\n",
    "        # Convert longitude to cell X coordinate (east)\n",
    "        if lon is None or lat is None:\n",
    "            return None\n",
    "        # Account for longitude distance variation with latitude\n",
    "        # Use math operations directly on float values\n",
    "        meters_east = (lon - ORIGIN_LON) * 111000 * math.cos(math.radians(lat))\n",
    "        cell_x = int(meters_east / CELL_SIZE) + 1\n",
    "        return cell_x if 1 <= cell_x <= MAX_CELL else None\n",
    "    \n",
    "    df = spark.readStream.format(\"delta\").load(SOURCE_DELTA_TABLE)\n",
    "    \n",
    "    # ingestion time in first read\n",
    "    df = df.withColumn(\"ingestion_time\", F.current_timestamp())\n",
    "    \n",
    "    # Ensure timestamp columns are properly formatted\n",
    "    clean_df = df.withColumn(\n",
    "        \"pickup_datetime\", F.to_timestamp(\"pickup_datetime\")\n",
    "    ).withColumn(\n",
    "        \"dropoff_datetime\", F.to_timestamp(\"dropoff_datetime\")\n",
    "    )\n",
    "    \n",
    "    # Apply the UDFs to convert coordinates to grid cells\n",
    "    routes_df = clean_df.withColumn(\n",
    "        \"start_cell_x\", lon_to_cell_x(F.col(\"pickup_longitude\"), F.col(\"pickup_latitude\"))\n",
    "    ).withColumn(\n",
    "        \"start_cell_y\", lat_to_cell_y(F.col(\"pickup_latitude\"))\n",
    "    ).withColumn(\n",
    "        \"end_cell_x\", lon_to_cell_x(F.col(\"dropoff_longitude\"), F.col(\"dropoff_latitude\"))\n",
    "    ).withColumn(\n",
    "        \"end_cell_y\", lat_to_cell_y(F.col(\"dropoff_latitude\"))\n",
    "    )\n",
    "    \n",
    "    # Filter outliers - cells must be within the grid (1,1) to (300,300)\n",
    "    valid_routes_df = routes_df.filter(\n",
    "        (F.col(\"start_cell_x\").isNotNull()) & \n",
    "        (F.col(\"start_cell_y\").isNotNull()) &\n",
    "        (F.col(\"end_cell_x\").isNotNull()) & \n",
    "        (F.col(\"end_cell_y\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # Create route identifiers in X.Y format\n",
    "    cell_routes_df = valid_routes_df.withColumn(\n",
    "        \"start_cell\", get_cell_id(F.col(\"start_cell_x\"), F.col(\"start_cell_y\"))\n",
    "    ).withColumn(\n",
    "        \"end_cell\", get_cell_id(F.col(\"end_cell_x\"), F.col(\"end_cell_y\"))\n",
    "    )\n",
    "    \n",
    "    # Create a 30-minute sliding window based on dropoff_datetime\n",
    "    # Using a sliding window with 5-minute slide to get more frequent updates\n",
    "    windowed_routes = cell_routes_df.withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "        .groupBy(\n",
    "            F.window(\"dropoff_datetime\", \"30 minutes\"),  # 30-min window sliding every 5 min\n",
    "            \"start_cell\", \n",
    "            \"end_cell\"\n",
    "        ) \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"Number_of_Rides\"),\n",
    "            F.max(\"pickup_datetime\").alias(\"latest_pickup\"),\n",
    "            F.max(\"dropoff_datetime\").alias(\"latest_dropoff\"),\n",
    "            F.max(\"ingestion_time\").alias(\"latest_ingestion\")\n",
    "        )\n",
    "    \n",
    "    # Reference for the previous top 10 routes\n",
    "    previous_top_routes = None\n",
    "    latest_window = None\n",
    "    def detect_changes_and_output(batch_df, batch_id):\n",
    "        \"\"\"\n",
    "        Process each micro-batch to detect changes in top 10 routes\n",
    "        and output updates when changes occur.\n",
    "        \"\"\"\n",
    "        nonlocal previous_top_routes\n",
    "        nonlocal latest_window\n",
    "\n",
    "        if batch_df.isEmpty():\n",
    "            return\n",
    "            \n",
    "        # Get the latest 30-minute window from the batch\n",
    "        # We only care about the most recent window for output\n",
    "        if latest_window is None:\n",
    "            window_ends = batch_df.select(\"window.end\").distinct().collect()\n",
    "            window_ends = [row[\"end\"] for row in window_ends]\n",
    "\n",
    "            if not window_ends:\n",
    "                return\n",
    "\n",
    "            # Sort the window ends\n",
    "            window_ends.sort()\n",
    "\n",
    "            # Calculate the 75th percentile index\n",
    "            percentile_idx = int(len(window_ends) * 0.75)\n",
    "            if percentile_idx >= len(window_ends):\n",
    "                percentile_idx = len(window_ends) - 1\n",
    "\n",
    "            # Get the window at the 75th percentile\n",
    "            latest_window = window_ends[percentile_idx]\n",
    "            print(latest_window)\n",
    "            # latest_window = batch_df.select(F.max(\"window.end\").alias(\"max_end\")).collect()[0][\"max_end\"]\n",
    "        \n",
    "        if latest_window is not None:\n",
    "            # Filter to data from the latest window\n",
    "            window_data = batch_df.filter(F.col(\"window.end\") == latest_window)\n",
    "            \n",
    "            # Get the top 10 routes by ride count\n",
    "            top_routes_df = window_data.orderBy(F.col(\"Number_of_Rides\").desc()).limit(10)\n",
    "            \n",
    "            # Convert to list of route tuples for comparison\n",
    "            current_top_routes = []\n",
    "            for row in top_routes_df.collect():\n",
    "                current_top_routes.append((\n",
    "                    row[\"start_cell\"],\n",
    "                    row[\"end_cell\"],\n",
    "                    row[\"Number_of_Rides\"],\n",
    "                    row[\"latest_pickup\"],\n",
    "                    row[\"latest_dropoff\"],\n",
    "                    row[\"latest_ingestion\"]\n",
    "                ))\n",
    "            \n",
    "            # Check if top routes have changed\n",
    "            routes_changed = False\n",
    "            \n",
    "            if previous_top_routes is None:\n",
    "                routes_changed = True\n",
    "            else:\n",
    "                # Compare current routes with previous ones\n",
    "                current_route_set = set((r[0], r[1]) for r in current_top_routes)\n",
    "                previous_route_set = set((r[0], r[1]) for r in previous_top_routes)\n",
    "                \n",
    "                if current_route_set != previous_route_set:\n",
    "                    routes_changed = True\n",
    "            \n",
    "            if routes_changed:\n",
    "                # We have a change - output the update\n",
    "                \n",
    "                # Find the latest event details (most recent dropoff)\n",
    "                latest_event = None\n",
    "                for route in current_top_routes:\n",
    "                    if route[4] is not None:  # latest_dropoff\n",
    "                        if latest_event is None or route[4] > latest_event[4]:\n",
    "                            latest_event = route\n",
    "                \n",
    "                if latest_event:\n",
    "                    # Extract timestamps for output\n",
    "                    pickup_time = latest_event[3]  # latest_pickup\n",
    "                    dropoff_time = latest_event[4]  # latest_dropoff\n",
    "                    ingestion_time = latest_event[5]  # latest_ingestion\n",
    "                    \n",
    "                    # Calculate delay between ingestion time and current processing time\n",
    "                    output_time = time.time()\n",
    "                    delay = output_time - ingestion_time.timestamp()\n",
    "                    \n",
    "                    # Prepare output data\n",
    "                    output_data = {\n",
    "                        \"pickup_datetime\": pickup_time,\n",
    "                        \"dropoff_datetime\": dropoff_time,\n",
    "                        \"delay\": delay\n",
    "                    }\n",
    "                    \n",
    "                    # Sort the routes to ensure consistent ordering by frequency\n",
    "                    sorted_routes = sorted(\n",
    "                        current_top_routes, \n",
    "                        key=lambda r: r[2], \n",
    "                        reverse=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add up to 10 routes to the output\n",
    "                    for i in range(1, 11):\n",
    "                        if i <= len(sorted_routes):\n",
    "                            route = sorted_routes[i-1]\n",
    "                            output_data[f\"start_cell_id_{i}\"] = route[0]\n",
    "                            output_data[f\"end_cell_id_{i}\"] = route[1]\n",
    "                        else:\n",
    "                            # Fill with NULL for missing routes\n",
    "                            output_data[f\"start_cell_id_{i}\"] = None\n",
    "                            output_data[f\"end_cell_id_{i}\"] = None\n",
    "                    \n",
    "                    # Create output DataFrame\n",
    "                    output_df = spark.createDataFrame([output_data], schema=output_schema)\n",
    "\n",
    "                    # Write to Delta table\n",
    "                    output_df.write.format(\"delta\").mode(\"append\").save(OUTPUT_DELTA_TABLE)\n",
    "                \n",
    "                # Update the reference for next comparison\n",
    "                previous_top_routes = current_top_routes\n",
    "    \n",
    "    # Set up the streaming query with foreachBatch\n",
    "    query = windowed_routes.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .foreachBatch(detect_changes_and_output) \\\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_LOCATION) \\\n",
    "        .start()\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4046311c-40c1-434f-ae3f-632da12dc2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 20:50:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-10-01 22:30:00\n"
     ]
    }
   ],
   "source": [
    "query_part_2 = query1_part2_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4eec28-9fc1-46ea-b7bc-0666d1873478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query_part_2.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3511732e-4bfe-483a-bb72-ea80ef1742be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = spark.read.format(\"delta\").load(FREQ_OUTPUT_PATH_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb0e86f-85e2-4fc0-96d2-42d753bde2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2013, 10, 1, 21, 41), dropoff_datetime=datetime.datetime(2013, 10, 1, 22, 27, 27), delay=76.60892987251282, start_cell_id_1='158.152', end_cell_id_1='157.155', start_cell_id_2='160.156', end_cell_id_2='159.158', start_cell_id_3='158.152', end_cell_id_3='161.156', start_cell_id_4='160.157', end_cell_id_4='156.159', start_cell_id_5='162.155', end_cell_id_5='162.156', start_cell_id_6='152.169', end_cell_id_6='157.156', start_cell_id_7='158.161', end_cell_id_7='160.160', start_cell_id_8='154.162', end_cell_id_8='154.167', start_cell_id_9='159.158', end_cell_id_9='162.155', start_cell_id_10='157.158', end_cell_id_10='155.158')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d48695d-eabc-4188-a16e-6be46b41208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = batch_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a24fed96-2dfb-44ca-8973-88d9038d9083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>delay</th>\n",
       "      <th>start_cell_id_1</th>\n",
       "      <th>end_cell_id_1</th>\n",
       "      <th>start_cell_id_2</th>\n",
       "      <th>end_cell_id_2</th>\n",
       "      <th>start_cell_id_3</th>\n",
       "      <th>end_cell_id_3</th>\n",
       "      <th>start_cell_id_4</th>\n",
       "      <th>...</th>\n",
       "      <th>start_cell_id_6</th>\n",
       "      <th>end_cell_id_6</th>\n",
       "      <th>start_cell_id_7</th>\n",
       "      <th>end_cell_id_7</th>\n",
       "      <th>start_cell_id_8</th>\n",
       "      <th>end_cell_id_8</th>\n",
       "      <th>start_cell_id_9</th>\n",
       "      <th>end_cell_id_9</th>\n",
       "      <th>start_cell_id_10</th>\n",
       "      <th>end_cell_id_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-10-01 18:41:00</td>\n",
       "      <td>2013-10-01 19:27:27</td>\n",
       "      <td>76.60893</td>\n",
       "      <td>158.152</td>\n",
       "      <td>157.155</td>\n",
       "      <td>160.156</td>\n",
       "      <td>159.158</td>\n",
       "      <td>158.152</td>\n",
       "      <td>161.156</td>\n",
       "      <td>160.157</td>\n",
       "      <td>...</td>\n",
       "      <td>152.169</td>\n",
       "      <td>157.156</td>\n",
       "      <td>158.161</td>\n",
       "      <td>160.160</td>\n",
       "      <td>154.162</td>\n",
       "      <td>154.167</td>\n",
       "      <td>159.158</td>\n",
       "      <td>162.155</td>\n",
       "      <td>157.158</td>\n",
       "      <td>155.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime     delay start_cell_id_1  \\\n",
       "0 2013-10-01 18:41:00 2013-10-01 19:27:27  76.60893         158.152   \n",
       "\n",
       "  end_cell_id_1 start_cell_id_2 end_cell_id_2 start_cell_id_3 end_cell_id_3  \\\n",
       "0       157.155         160.156       159.158         158.152       161.156   \n",
       "\n",
       "  start_cell_id_4  ... start_cell_id_6 end_cell_id_6 start_cell_id_7  \\\n",
       "0         160.157  ...         152.169       157.156         158.161   \n",
       "\n",
       "  end_cell_id_7 start_cell_id_8 end_cell_id_8 start_cell_id_9 end_cell_id_9  \\\n",
       "0       160.160         154.162       154.167         159.158       162.155   \n",
       "\n",
       "  start_cell_id_10 end_cell_id_10  \n",
       "0          157.158        155.158  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
