{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c16de-63b9-4d72-83b2-d9eaab099990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Remove the empty space in each cell of the header, schema parsing was erroring.\n",
    "# If erroring run it from the shell.\n",
    "# !for file in input/trip_data_*.csv; do sed -i '1s/, /,/g' \"$file\"; done\n",
    "# !for file in input/sample.csv; do sed -i '1s/, /,/g' \"$file\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90428315-cc8b-4f0b-b2e6-c15f864471ea",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7c269-03e6-4830-861f-35700ab1ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, to_json, struct, lit, current_timestamp, expr, when, from_json\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff745b-b0fe-4fde-b85d-3997fac09a4c",
   "metadata": {},
   "source": [
    "## Schema definition fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef38df3-87a6-49c9-9fe7-4515a24e61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a consistent warehouse directory path - use absolute path\n",
    "WAREHOUSE_DIR = \"/home/jovyan/spark-warehouse\"\n",
    "\n",
    "# Create the schema definition for NYC taxi data\n",
    "def create_taxi_schema():\n",
    "    \"\"\"Create the schema for NYC taxi data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"medallion\", StringType(), True),\n",
    "        StructField(\"hack_license\", StringType(), True),\n",
    "        StructField(\"vendor_id\", StringType(), True),\n",
    "        StructField(\"rate_code\", StringType(), True),\n",
    "        StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"passenger_count\", IntegerType(), True),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29307dd0-630e-41ad-bfc6-4e34ab70d121",
   "metadata": {},
   "source": [
    "## Spark Session fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d1e1e-8103-42d5-993a-f7d304ecb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"NYC Taxi Data ETL\"):\n",
    "    \"\"\"\n",
    "    start spark session with kafka and delta support / memory config setup too\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder.appName(app_name) \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", WAREHOUSE_DIR) \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.driver.memory\", \"5g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "        .enableHiveSupport()\n",
    "    \n",
    "    # delta config\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # do not flood logs\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    # Print configs for debugging\n",
    "    print(f\"Warehouse directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "    print(f\"Catalog implementation: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f145391-d837-456c-a208-c0bb967c42d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse directory: file:/home/jovyan/spark-warehouse\n",
      "Catalog implementation: hive\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5136e-4a75-41e1-9307-bbf402d7b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the taxi schema\n",
    "raw_taxi_schema = create_taxi_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650c28d-ad88-4adc-8f66-460659f9d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ingest CSV data to Kafka\n",
    "def ingest_csv_to_kafka(csv_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Read a CSV file and publish records to Kafka using Spark's partitioning\n",
    "    \"\"\"\n",
    "    print(f\"Ingesting data from {csv_path} to Kafka topic 'raw-taxi-data'\")\n",
    "    \n",
    "    # Load the data\n",
    "    taxi_data = spark.read.csv(csv_path, header=True, schema=raw_taxi_schema)\n",
    "    \n",
    "    # Get total count for reporting\n",
    "    total_count = taxi_data.count()\n",
    "    print(f\"Total records to process: {total_count}\")\n",
    "    \n",
    "    # Add partition key\n",
    "    taxi_data = taxi_data.withColumn(\"kafka_key\", col(\"medallion\"))\n",
    "    \n",
    "    # Create the JSON structure\n",
    "    kafka_batch = taxi_data.select(\n",
    "        col(\"kafka_key\").cast(\"string\"),\n",
    "        to_json(\n",
    "            struct(*[col(c) for c in taxi_data.columns if c != \"kafka_key\"])\n",
    "        ).alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    # Write to Kafka in one go (Spark will handle the batching internally)\n",
    "    kafka_batch.write.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"topic\", \"raw-taxi-data\") \\\n",
    "        .option(\"maxOffsetsPerTrigger\", batch_size) \\\n",
    "        .save()\n",
    "    \n",
    "    print(f\"Finished ingesting data to Kafka topic 'raw-taxi-data'\")\n",
    "\n",
    "# Function to create a table once streaming data is available\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    \"\"\"\n",
    "    Check if data exists in the given path and create a table pointing to it\n",
    "    \"\"\"\n",
    "    data_exists = False\n",
    "    for _i in range(30):  # Longer timeout to 30 seconds\n",
    "        try:\n",
    "            time.sleep(120)\n",
    "            if os.path.exists(output_path):\n",
    "                files = os.listdir(output_path)\n",
    "                for _f in files:\n",
    "                    if \".parquet\" in _f:\n",
    "                        if os.path.exists(f\"{output_path}/_delta_log\") and len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                            print(f\"Data exists in {output_path}\")\n",
    "                            data_exists = True\n",
    "                            break\n",
    "            if data_exists:\n",
    "                # Create external table with explicit location\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "                spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                print(f\"Created table {table_name} using data at {output_path}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Waiting for data: {e}\")\n",
    "            pass\n",
    "    \n",
    "    if not data_exists:\n",
    "        print(f\"WARNING: No data found in {output_path} after waiting. Table may not be created.\")\n",
    "\n",
    "# Function to clean the taxi data\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def clean_taxi_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and filters the taxi data stream by removing invalid trips, \n",
    "    handling nulls, and computing additional features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define valid range conditions for latitude and longitude\n",
    "    valid_coords = (\n",
    "        (col(\"pickup_longitude\").between(-180, 180)) & \n",
    "        (col(\"pickup_latitude\").between(-90, 90)) & \n",
    "        (col(\"dropoff_longitude\").between(-180, 180)) & \n",
    "        (col(\"dropoff_latitude\").between(-90, 90))\n",
    "    )\n",
    "\n",
    "    # Filter invalid trips\n",
    "    cleaned_df = df.filter(\n",
    "        col(\"medallion\").isNotNull() &   # Ensure taxi identifier exists\n",
    "        col(\"trip_distance\") > 0 &       # Trips must have distance\n",
    "        col(\"passenger_count\") > 0 &     # Must have at least one passenger\n",
    "        col(\"trip_time_in_secs\") > 0 &   # Duration must be positive\n",
    "        valid_coords                     # Coordinates must be valid\n",
    "    )\n",
    "\n",
    "    # Compute trip speed in mph\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        \"trip_speed_mph\", \n",
    "        when(col(\"trip_time_in_secs\") > 0, \n",
    "             col(\"trip_distance\") / (col(\"trip_time_in_secs\") / 3600)\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Flag valid trips based on speed\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        \"is_valid_trip\", \n",
    "        (col(\"trip_distance\") > 0) & \n",
    "        (col(\"trip_time_in_secs\") > 0) & \n",
    "        (col(\"trip_speed_mph\") < 100)  # Filter unrealistic speeds\n",
    "    )\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4416899-e27b-4edd-9a56-2374520c2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define paths\n",
    "RAW_TABLE_NAME = \"raw_taxi_data\"\n",
    "CLEAN_TABLE_NAME = \"clean_taxi_data\"\n",
    "\n",
    "RAW_OUTPUT_PATH = os.path.join(WAREHOUSE_DIR, RAW_TABLE_NAME)\n",
    "CLEAN_OUTPUT_PATH = os.path.join(WAREHOUSE_DIR, CLEAN_TABLE_NAME)\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(WAREHOUSE_DIR, \"streaming/checkpoints\")\n",
    "RAW_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"raw_taxi_data\") \n",
    "CLEAN_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"clean_taxi_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da156543-d63f-4db1-af86-9faa46fea738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure checkpoint directories exist\n",
    "os.makedirs(RAW_CHECKPOINT_PATH, exist_ok=True)\n",
    "os.makedirs(CLEAN_CHECKPOINT_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a90dfdb-baa3-4e0d-86fc-25bb9df6fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ingest CSV data to Kafka (producer)\n",
    "def run_producer():\n",
    "    csv_path = \"input/trip_data_8.csv\"\n",
    "    ingest_csv_to_kafka(csv_path, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe8dbbc-644d-419d-96bd-85504cd4f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up the consumer to read from Kafka and write to Delta table\n",
    "def run_consumer(skip_table_creation=False):\n",
    "    # 1. Read from Kafka\n",
    "    raw_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "        .option(\"subscribe\", \"raw-taxi-data\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "    \n",
    "    # 2. Parse JSON data\n",
    "    filtered_kafka_stream = raw_stream.select(\n",
    "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "        col(\"value\").cast(\"string\").alias(\"json_data\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    \n",
    "    raw_data_df = filtered_kafka_stream.select(\n",
    "        \"kafka_key\",\n",
    "        from_json(\"json_data\", raw_taxi_schema).alias(\"data\"),\n",
    "        \"kafka_timestamp\"\n",
    "    ).select(\"kafka_key\", \"kafka_timestamp\", \"data.*\")\n",
    "    \n",
    "    # 3. Write raw data to Delta\n",
    "    raw_query = (raw_data_df.writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .queryName(\"raw_taxi_query\")\n",
    "        .trigger(processingTime=\"10 second\")\n",
    "        .option(\"checkpointLocation\", RAW_CHECKPOINT_PATH)\n",
    "        .start(RAW_OUTPUT_PATH)\n",
    "    )\n",
    "    \n",
    "    # Wait for some data to be written\n",
    "    print(\"Waiting for raw data to be processed...\")\n",
    "    time.sleep(60)\n",
    "    \n",
    "    # 4. Create table in the metastore\n",
    "    if not skip_table_creation:\n",
    "        create_table_if_exists(RAW_OUTPUT_PATH, RAW_TABLE_NAME)\n",
    "    \n",
    "    return raw_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e340dd-a227-4c7e-ae3e-d768f7682ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Clean data pipeline\n",
    "def run_cleaner(raw_query):\n",
    "    # 1. Read from raw Delta table\n",
    "    raw_df = spark.readStream.format(\"delta\").load(RAW_OUTPUT_PATH)\n",
    "    \n",
    "    # 2. Apply cleaning transformations\n",
    "    clean_df = clean_taxi_data(raw_df)\n",
    "    \n",
    "    # 3. Write clean data to Delta\n",
    "    clean_query = (clean_df.writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .queryName(\"clean_taxi_query\")\n",
    "        .trigger(processingTime=\"10 second\")\n",
    "        .option(\"checkpointLocation\", CLEAN_CHECKPOINT_PATH)\n",
    "        .start(CLEAN_OUTPUT_PATH)\n",
    "    )\n",
    "    \n",
    "    # Wait for some data to be written\n",
    "    print(\"Waiting for clean data to be processed...\")\n",
    "    time.sleep(30)\n",
    "    \n",
    "    # 4. Create table in the metastore\n",
    "    create_table_if_exists(CLEAN_OUTPUT_PATH, CLEAN_TABLE_NAME)\n",
    "    \n",
    "    return clean_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33a80ce-c23f-474c-8d52-f284ce4765dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Verify tables\n",
    "def verify_tables():\n",
    "    print(\"Available tables:\")\n",
    "    spark.sql(\"SHOW TABLES\").show()\n",
    "    \n",
    "    print(\"\\nRaw table schema:\")\n",
    "    spark.sql(f\"DESCRIBE EXTENDED {RAW_TABLE_NAME}\").show(truncate=False)\n",
    "    \n",
    "    print(\"\\nClean table schema:\")\n",
    "    spark.sql(f\"DESCRIBE EXTENDED {CLEAN_TABLE_NAME}\").show(truncate=False)\n",
    "    \n",
    "    print(\"\\nSample data from raw table:\")\n",
    "    spark.sql(f\"SELECT * FROM {RAW_TABLE_NAME} LIMIT 5\").show()\n",
    "    \n",
    "    print(\"\\nSample data from clean table:\")\n",
    "    spark.sql(f\"SELECT * FROM {CLEAN_TABLE_NAME} LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09b244-8289-4d6a-9555-2d13107030bc",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649e9c0-9bb8-4fea-a182-9e51d841e0df",
   "metadata": {},
   "source": [
    "The data is moved from `csv -> kafka topic`. In this part since we produce the data we use batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8434e4-9f46-4757-a534-293e25000efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the producer\n",
    "run_producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79ce0c-86dc-430d-9ba6-f3b720f610a7",
   "metadata": {},
   "source": [
    "We consume the data later on and get the streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830ca5a-1406-4e4f-ac8f-6720c04c0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the consumer\n",
    "raw_query = run_consumer(skip_table_creation=False) # default should be False, if run from scratch change it to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92ba79-0329-4fd1-90a5-dd8dadcc2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cleaner\n",
    "clean_query = run_cleaner(raw_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8665042-4d35-4f36-b5c4-8f95fff2fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tables\n",
    "verify_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539cd6f9-c0ce-401c-a855-11da8fcc3e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for raw data to be processed...\n",
      "Data exists in /home/jovyan/spark-warehouse/raw_taxi_data\n",
      "Created table raw_taxi_data using data at /home/jovyan/spark-warehouse/raw_taxi_data\n",
      "Waiting for clean data to be processed...\n",
      "Data exists in /home/jovyan/spark-warehouse/clean_taxi_data\n",
      "Created table clean_taxi_data using data at /home/jovyan/spark-warehouse/clean_taxi_data\n",
      "Available tables:\n",
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  default|clean_taxi_data|      false|\n",
      "|  default|  raw_taxi_data|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n",
      "\n",
      "Raw table schema:\n",
      "+----------------------------+-----------------------------------+-------+\n",
      "|col_name                    |data_type                          |comment|\n",
      "+----------------------------+-----------------------------------+-------+\n",
      "|kafka_key                   |string                             |NULL   |\n",
      "|kafka_timestamp             |timestamp                          |NULL   |\n",
      "|medallion                   |string                             |NULL   |\n",
      "|hack_license                |string                             |NULL   |\n",
      "|vendor_id                   |string                             |NULL   |\n",
      "|rate_code                   |string                             |NULL   |\n",
      "|store_and_fwd_flag          |string                             |NULL   |\n",
      "|pickup_datetime             |timestamp                          |NULL   |\n",
      "|dropoff_datetime            |timestamp                          |NULL   |\n",
      "|passenger_count             |int                                |NULL   |\n",
      "|trip_time_in_secs           |int                                |NULL   |\n",
      "|trip_distance               |double                             |NULL   |\n",
      "|pickup_longitude            |double                             |NULL   |\n",
      "|pickup_latitude             |double                             |NULL   |\n",
      "|dropoff_longitude           |double                             |NULL   |\n",
      "|dropoff_latitude            |double                             |NULL   |\n",
      "|                            |                                   |       |\n",
      "|# Detailed Table Information|                                   |       |\n",
      "|Name                        |spark_catalog.default.raw_taxi_data|       |\n",
      "|Type                        |EXTERNAL                           |       |\n",
      "+----------------------------+-----------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Clean table schema:\n",
      "+----------------------------+---------+-------+\n",
      "|col_name                    |data_type|comment|\n",
      "+----------------------------+---------+-------+\n",
      "|kafka_key                   |string   |NULL   |\n",
      "|kafka_timestamp             |timestamp|NULL   |\n",
      "|medallion                   |string   |NULL   |\n",
      "|hack_license                |string   |NULL   |\n",
      "|vendor_id                   |string   |NULL   |\n",
      "|rate_code                   |string   |NULL   |\n",
      "|store_and_fwd_flag          |string   |NULL   |\n",
      "|pickup_datetime             |timestamp|NULL   |\n",
      "|dropoff_datetime            |timestamp|NULL   |\n",
      "|passenger_count             |int      |NULL   |\n",
      "|trip_time_in_secs           |int      |NULL   |\n",
      "|trip_distance               |double   |NULL   |\n",
      "|pickup_longitude            |double   |NULL   |\n",
      "|pickup_latitude             |double   |NULL   |\n",
      "|dropoff_longitude           |double   |NULL   |\n",
      "|dropoff_latitude            |double   |NULL   |\n",
      "|trip_speed_mph              |double   |NULL   |\n",
      "|is_valid_trip               |boolean  |NULL   |\n",
      "|                            |         |       |\n",
      "|# Detailed Table Information|         |       |\n",
      "+----------------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Sample data from raw table:\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|kafka_key|     kafka_timestamp|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|    pickup_datetime|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|     NULL|2025-03-27 17:12:...|3418135604CD3F357...|B25386A1F259C8744...|      VTS|        1|              NULL|2013-08-30 07:57:00|2013-08-30 08:30:00|              5|             1980|        14.58|      -73.791359|      40.645657|       -73.922501|       40.758766|\n",
      "|     NULL|2025-03-27 17:12:...|B341CD110148050E6...|F73AA91687B1C2E23...|      VTS|        1|              NULL|2013-08-25 16:26:00|2013-08-25 16:36:00|              1|              600|         1.24|      -73.986221|      40.759483|       -73.995247|       40.749817|\n",
      "|     NULL|2025-03-27 17:12:...|CD6A088A792463164...|C3A87F0847E12D914...|      CMT|        1|                 N|2013-08-02 08:01:40|2013-08-02 08:05:10|              1|              209|          0.3|      -73.982117|      40.749187|        -73.97892|       40.752605|\n",
      "|     NULL|2025-03-27 17:12:...|4F34ACD42D64890E1...|58A7E1A66282EA656...|      VTS|        1|              NULL|2013-08-18 16:15:00|2013-08-18 16:35:00|              1|             1200|         3.22|      -73.971603|      40.765316|        -73.98819|       40.757427|\n",
      "|     NULL|2025-03-27 17:12:...|09D95C058609B6396...|CE3675661661E1289...|      VTS|        1|              NULL|2013-08-25 16:32:00|2013-08-25 16:35:00|              1|              180|         0.45|      -73.952667|      40.778446|       -73.957184|       40.778542|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "Sample data from clean table:\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------------+-------------+\n",
      "|kafka_key|     kafka_timestamp|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|    pickup_datetime|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|    trip_speed_mph|is_valid_trip|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------------+-------------+\n",
      "|     NULL|2025-03-27 17:14:...|9F3B5F7A9F37581A5...|4AABE4BFCF23432BC...|      VTS|        1|              NULL|2013-08-23 20:22:00|2013-08-23 20:36:00|              6|              840|         3.79|      -73.966606|      40.792568|        -73.96769|       40.753281|16.242857142857144|         true|\n",
      "|     NULL|2025-03-27 17:14:...|21AF1D87DCDA4F7A5...|E9A57E5281220E6E8...|      VTS|        1|              NULL|2013-08-11 02:39:00|2013-08-11 02:43:00|              3|              240|         0.68|      -73.999634|      40.728249|       -73.989883|       40.723221|10.200000000000001|         true|\n",
      "|     NULL|2025-03-27 17:14:...|374553F6D30888FCA...|3F5A7FEF1FA7E8B99...|      VTS|        1|              NULL|2013-08-20 16:19:00|2013-08-20 16:42:00|              1|             1380|         3.06|      -73.963737|      40.777161|       -73.991364|       40.739243| 7.982608695652173|         true|\n",
      "|     NULL|2025-03-27 17:14:...|AC07C55C25773A929...|2636A1522A36B8CAE...|      VTS|        1|              NULL|2013-08-11 02:33:00|2013-08-11 02:45:00|              5|              720|         3.06|      -73.984734|      40.764099|       -73.949074|       40.777271|15.299999999999999|         true|\n",
      "|     NULL|2025-03-27 17:14:...|091334CDF9A2292C0...|58B7B1C9E2A7BA3F2...|      VTS|        1|              NULL|2013-08-20 16:19:00|2013-08-20 16:39:00|              1|             1200|         1.09|      -73.994011|      40.724758|       -73.972069|       40.763252|3.2700000000000005|         true|\n",
      "+---------+--------------------+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------------+-------------+\n",
      "\n",
      "\n",
      "Raw query status:\n",
      "{'message': 'Terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Is active: False\n",
      "\n",
      "Clean query status:\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Is active: True\n"
     ]
    }
   ],
   "source": [
    "# Print query status\n",
    "print(\"\\nRaw query status:\")\n",
    "print(raw_query.status)\n",
    "print(f\"Is active: {raw_query.isActive}\")\n",
    "\n",
    "print(\"\\nClean query status:\")\n",
    "print(clean_query.status)\n",
    "print(f\"Is active: {clean_query.isActive}\")\n",
    "\n",
    "# raw_query.stop()\n",
    "# clean_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc9efe9-4fb9-4834-b02c-fea39006a319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query name: clean_taxi_query\n",
      "Status: {'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Is active: True\n"
     ]
    }
   ],
   "source": [
    "for query in spark.streams.active:\n",
    "    print(f\"Query name: {query.name}\")\n",
    "    print(f\"Status: {query.status}\")\n",
    "    print(f\"Is active: {query.isActive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5274c64-f1e0-4e59-9daa-45ed2ec44334",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_query.stop()\n",
    "clean_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d231b-9037-4c72-a2a2-a712f551b522",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973345e-b2df-492a-88b9-072a27e0061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "We already have the tables persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f34f568-74a5-4b9e-aa55-d80cec1a4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.table(RAW_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7e52ae-cd64-46fb-94f7-f1e649f0a007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50388436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d615c283-7400-4f3e-b499-da4c9d5a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = spark.table(CLEAN_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51fdd5b9-ced7-4470-b163-33fec1449134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49208520"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
