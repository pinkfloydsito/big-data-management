{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3565b05b-59bd-4645-b24c-f31f2ac7ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, window, unix_timestamp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2484dc30-5360-48ba-878c-b1556af6d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session (for Jupyter Notebook)\n",
    "spark = SparkSession.builder.appName(\"FrequentRoutes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2409a3f1-888c-4f98-893c-c0108e0342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"Data/cleaned_data.parquet/cleaned_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5746b52d-0609-4948-ad73-6893c10c3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|kafka_key|     kafka_timestamp|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|     NULL|2025-03-28 17:45:...|6C8C5507F1928059F...|10AC7E695DB02A51B...|2013-05-07 18:15:53|2013-05-07 18:22:48|              414|          1.0|      -73.970161|      40.765541|        -73.96077|       40.778202|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|         9.6|\n",
      "|     NULL|2025-03-28 17:45:...|E8E54922CFDFC4808...|BCCAFB9AB09496225...|2013-10-05 13:53:00|2013-10-05 14:05:00|              720|         1.32|      -74.008781|      40.714012|       -73.991737|       40.715076|         CRD|        9.0|      0.0|    0.5|       1.5|         0.0|        11.0|\n",
      "|     NULL|2025-03-28 17:45:...|F0CC93FCBADFAB61B...|304600A94411F02AA...|2013-05-07 18:18:47|2013-05-07 18:22:56|              248|          1.0|       -73.98307|      40.738518|       -73.986572|       40.726372|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|         7.0|\n",
      "|     NULL|2025-03-28 17:45:...|41636154FEC252058...|8A7E6CA163C79FAF3...|2013-10-05 13:46:36|2013-10-05 14:05:08|             1111|          1.7|      -73.972557|      40.756912|       -73.992561|       40.750443|         CSH|       12.5|      0.0|    0.5|       0.0|         0.0|        13.0|\n",
      "|     NULL|2025-03-28 17:45:...|0B8862989540F861B...|FCC9C532C82403D53...|2013-05-07 18:13:00|2013-05-07 18:23:00|              600|         1.67|      -73.992172|      40.730038|       -73.978523|        40.75066|         CSH|        8.5|      1.0|    0.5|       0.0|         0.0|        10.0|\n",
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f488700-fcae-4ce5-a157-94265d22e1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+------------+\n",
      "|kafka_key|     kafka_timestamp|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|dropoff_time|\n",
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+------------+\n",
      "|     NULL|2025-03-28 17:45:...|6C8C5507F1928059F...|10AC7E695DB02A51B...|2013-05-07 18:15:53|2013-05-07 18:22:48|              414|          1.0|      -73.970161|      40.765541|        -73.96077|       40.778202|         CRD|        6.5|      1.0|    0.5|       1.6|         0.0|         9.6|  1367950968|\n",
      "|     NULL|2025-03-28 17:45:...|E8E54922CFDFC4808...|BCCAFB9AB09496225...|2013-10-05 13:53:00|2013-10-05 14:05:00|              720|         1.32|      -74.008781|      40.714012|       -73.991737|       40.715076|         CRD|        9.0|      0.0|    0.5|       1.5|         0.0|        11.0|  1380981900|\n",
      "|     NULL|2025-03-28 17:45:...|F0CC93FCBADFAB61B...|304600A94411F02AA...|2013-05-07 18:18:47|2013-05-07 18:22:56|              248|          1.0|       -73.98307|      40.738518|       -73.986572|       40.726372|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|         7.0|  1367950976|\n",
      "|     NULL|2025-03-28 17:45:...|41636154FEC252058...|8A7E6CA163C79FAF3...|2013-10-05 13:46:36|2013-10-05 14:05:08|             1111|          1.7|      -73.972557|      40.756912|       -73.992561|       40.750443|         CSH|       12.5|      0.0|    0.5|       0.0|         0.0|        13.0|  1380981908|\n",
      "|     NULL|2025-03-28 17:45:...|0B8862989540F861B...|FCC9C532C82403D53...|2013-05-07 18:13:00|2013-05-07 18:23:00|              600|         1.67|      -73.992172|      40.730038|       -73.978523|        40.75066|         CSH|        8.5|      1.0|    0.5|       0.0|         0.0|        10.0|  1367950980|\n",
      "+---------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out null values and invalid trips\n",
    "df = df.filter(\n",
    "    (col(\"pickup_longitude\").isNotNull()) & (col(\"pickup_latitude\").isNotNull()) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) & (col(\"dropoff_latitude\").isNotNull()) &\n",
    "    (col(\"trip_time_in_secs\") > 0) & (col(\"trip_distance\") > 0)\n",
    ")\n",
    "\n",
    "# Convert dropoff_datetime to a timestamp\n",
    "df = df.withColumn(\"dropoff_time\", unix_timestamp(col(\"dropoff_datetime\")))\n",
    "\n",
    "# Show cleaned data\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907db524-b49a-4c89-97ef-3ef60e4b509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to compute grid cell ID\n",
    "def get_cell_id(lat, lon):\n",
    "    base_lat, base_lon = 41.474937, -74.913585  # NYC reference point\n",
    "    cell_size = 0.0045  # Approx. 500m in degrees\n",
    "\n",
    "    cell_x = int((lon - base_lon) / cell_size) + 1\n",
    "    cell_y = int((base_lat - lat) / cell_size) + 1\n",
    "    return f\"{cell_x}.{cell_y}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fde3f9-2a71-4bf3-9c9e-926cfdbba1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|start_cell|end_cell|\n",
      "+----------+--------+\n",
      "|   210.158| 212.155|\n",
      "|   202.170| 205.169|\n",
      "|   207.164| 207.167|\n",
      "|   210.160| 205.161|\n",
      "|   205.166| 208.161|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register UDF\n",
    "grid_udf = udf(get_cell_id, StringType())\n",
    "\n",
    "# Add columns for start and end grid cells\n",
    "df = df.withColumn(\"start_cell\", grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "df = df.withColumn(\"end_cell\", grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "# Show updated dataframe with grid cells\n",
    "df.select(\"start_cell\", \"end_cell\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b967eeb4-4678-4d83-891a-43e8035282a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, max, window, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa7cca2-dfb9-4fe5-b255-30626e6cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dropoff_time to timestamp\n",
    "from pyspark.sql.functions import col, count, window, from_unixtime\n",
    "df = df.withColumn(\"dropoff_time\", from_unixtime(col(\"dropoff_time\")).cast(\"timestamp\"))\n",
    "\n",
    "# Step 1: Find the time window with the most rides\n",
    "windowed_counts = df.groupBy(window(col(\"dropoff_time\"), \"30 minutes\")).agg(count(\"*\").alias(\"num_rides\"))\n",
    "# Get the window with the highest number of rides\n",
    "max_rides_window = windowed_counts.orderBy(col(\"num_rides\").desc()).limit(1).collect()[0][0]\n",
    "\n",
    "# Extract start and end of that time window\n",
    "window_start, window_end = max_rides_window[\"start\"], max_rides_window[\"end\"]\n",
    "# Step 2: Filter rides that happened in this peak 30-minute window\n",
    "peak_time_df = df.filter((col(\"dropoff_time\") >= window_start) & (col(\"dropoff_time\") < window_end))\n",
    "\n",
    "# Step 3: Group by start_cell and end_cell, counting the number of rides\n",
    "routes = peak_time_df.groupBy(col(\"start_cell\"), col(\"end_cell\")).agg(count(\"*\").alias(\"num_rides\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9409ac2e-5ec8-4df2-93ca-ed7a63c92ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter out routes where start_cell == end_cell\n",
    "filtered_routes = routes.filter(col(\"start_cell\") != col(\"end_cell\"))\n",
    "\n",
    "# Step 5: Get top 10 most frequent routes\n",
    "top_routes = filtered_routes.orderBy(col(\"num_rides\").desc()).limit(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6dc27c4-a834-4c6c-98d3-51e7424a43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|start_cell|end_cell|num_rides|\n",
      "+----------+--------+---------+\n",
      "|   206.165| 208.163|        3|\n",
      "|   206.165| 209.162|        3|\n",
      "|   207.162| 208.166|        3|\n",
      "|   206.165| 205.162|        2|\n",
      "|   204.168| 204.164|        2|\n",
      "|   206.168| 202.171|        2|\n",
      "|   204.168| 206.162|        2|\n",
      "|   205.161| 207.164|        2|\n",
      "|   206.168| 207.165|        2|\n",
      "|   205.161| 207.157|        2|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "top_routes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a56bddd-98e7-4a21-bfc7-a33c2e58a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def query1_part2_batch(df):\n",
    "    \"\"\"Batch implementation of Part 2 requirements\"\"\"\n",
    "    \n",
    "    # 1. Prepare data with timestamps\n",
    "    df = df.withColumn(\"dropoff_ts\", col(\"dropoff_datetime\").cast(\"timestamp\"))\n",
    "    \n",
    "    # 2. Create time reference points every 1 minute (adjustable)\n",
    "    min_time = df.select(min(\"dropoff_ts\")).first()[0]\n",
    "    max_time = df.select(max(\"dropoff_ts\")).first()[0]\n",
    "    \n",
    "    # Generate time points at 1-minute intervals\n",
    "    time_points = spark.range(\n",
    "        int(min_time.timestamp()),\n",
    "        int(max_time.timestamp()) + 60,\n",
    "        60  # 1-minute intervals\n",
    "    ).select(\n",
    "        from_unixtime(\"id\").alias(\"processing_time\")\n",
    "    )\n",
    "    \n",
    "    # 3. Join trips to time points (30-minute windows)\n",
    "    windowed = df.join(\n",
    "        time_points,\n",
    "        (col(\"dropoff_ts\") >= col(\"processing_time\") - expr(\"INTERVAL 30 MINUTES\")) & \n",
    "        (col(\"dropoff_ts\") <= col(\"processing_time\"))\n",
    "    )\n",
    "    \n",
    "    # 4. Count routes per time window\n",
    "    route_counts = windowed.groupBy(\n",
    "        \"processing_time\", \"start_cell\", \"end_cell\"\n",
    "    ).agg(\n",
    "        count(\"*\").alias(\"num_rides\")\n",
    "    )\n",
    "    \n",
    "    # 5. Rank routes within each window\n",
    "    window_spec = Window.partitionBy(\"processing_time\").orderBy(col(\"num_rides\").desc())\n",
    "    ranked_routes = route_counts.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "                              .filter(col(\"rank\") <= 10)\n",
    "    \n",
    "    # 6. Pivot to get the required output format\n",
    "    pivot_df = ranked_routes.groupBy(\"processing_time\").pivot(\"rank\", range(1,11)).agg(\n",
    "        first(\"start_cell\").alias(\"start_cell\"),\n",
    "        first(\"end_cell\").alias(\"end_cell\")\n",
    "    )\n",
    "    \n",
    "    # 7. Format final output\n",
    "    output_cols = [\"processing_time\"]\n",
    "    for i in range(1,11):\n",
    "        output_cols.extend([\n",
    "            col(f\"{i}_start_cell\").alias(f\"start_cell_id_{i}\"),\n",
    "            col(f\"{i}_end_cell\").alias(f\"end_cell_id_{i}\")\n",
    "        ])\n",
    "    \n",
    "    result = pivot_df.select(*output_cols).withColumn(\"delay\", lit(0))\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c6b9191-8ac6-4ff6-9803-40815cd76aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch processing\n",
    "part2_results = query1_part2_batch(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246894ac-35f8-4296-b9e1-2ee7e4712824",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_results.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f4131-a6bf-437b-bade-e53e67c7644e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
