{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615b9793-7dcf-446c-bc9d-c2eb13ad85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, expr, when, lit, array, percentile_approx, desc\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType,\n",
    "    DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, window, unix_timestamp, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, to_json, struct, lit, current_timestamp, expr, when, from_json, window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e23a35-f1e4-4259-bf84-5cd6479bbe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/gpfs/helios/home/fidankarimova/myenv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /gpfs/helios/home/fidankarimova/.ivy2/cache\n",
      "The jars for the packages stored in: /gpfs/helios/home/fidankarimova/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-630e1920-9d42-4a94-a611-d5306931a5f9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-630e1920-9d42-4a94-a611-d5306931a5f9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/14ms)\n",
      "25/03/30 17:42:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse directory: file:/gpfs/helios/home/fidankarimova/input/spark-warehouse/spark-warehouse\n",
      "Catalog implementation: hive\n"
     ]
    }
   ],
   "source": [
    "WAREHOUSE_DIR = \"./spark-warehouse\"\n",
    "\n",
    "def create_spark_session(app_name=\"FrequentRoutes\"):\n",
    "    \"\"\"\n",
    "    start spark session with kafka and delta support / memory config setup too\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder.appName(app_name) \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", WAREHOUSE_DIR) \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.driver.memory\", \"5g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "        .enableHiveSupport()\n",
    "    \n",
    "    # delta config\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # do not flood logs\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    # Print configs for debugging\n",
    "    print(f\"Warehouse directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "    print(f\"Catalog implementation: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d6d40a-dac3-4fa9-9022-079cf2db8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Define Schema and Read Cleaned Data as a Stream\n",
    "#######################################\n",
    "def create_raw_taxi_schema():\n",
    "    return StructType([\n",
    "        StructField(\"medallion\", StringType(), True),\n",
    "        StructField(\"hack_license\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "        StructField(\"payment_type\", StringType(), True),\n",
    "        StructField(\"fare_amount\", DoubleType(), True),\n",
    "        StructField(\"surcharge\", DoubleType(), True),\n",
    "        StructField(\"mta_tax\", DoubleType(), True),\n",
    "        StructField(\"tip_amount\", DoubleType(), True),\n",
    "        StructField(\"tolls_amount\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "raw_schema = create_raw_taxi_schema()\n",
    "CLEAN_OUTPUT_PATH = \"clean_taxi_data\"\n",
    "batch_df = spark.read.format(\"delta\").load(CLEAN_OUTPUT_PATH)\n",
    "df = spark.readStream.format(\"delta\").load(CLEAN_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26a1c08-6cf9-4d29-ac15-5b12d6ca0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, ArrayType\n",
    "\n",
    "LAT_REF = 41.474937\n",
    "LON_REF = -74.913585\n",
    "EARTH_RADIUS = 6371000.0  # in meters\n",
    "\n",
    "def latlon_to_meters(lat, lon):\n",
    "    \"\"\"Approx x,y distance from reference using equirectangular approximation.\"\"\"\n",
    "    lat_r = math.radians(lat)\n",
    "    lon_r = math.radians(lon)\n",
    "    lat_ref_r = math.radians(LAT_REF)\n",
    "    lon_ref_r = math.radians(LON_REF)\n",
    "    x = EARTH_RADIUS * (lon_r - lon_ref_r) * math.cos((lat_r + lat_ref_r) / 2)\n",
    "    y = EARTH_RADIUS * (lat_r - lat_ref_r)\n",
    "    return (x, y)\n",
    "\n",
    "def get_250m_cell(lat, lon):\n",
    "    \"\"\"Return [cell_x, cell_y] for a 250m x 250m grid. None if out of range.\"\"\"\n",
    "    (x_m, y_m) = latlon_to_meters(lat, lon)\n",
    "    cell_x = int(math.floor(x_m / 250.0)) + 1\n",
    "    cell_y = int(math.floor((-1 * y_m) / 250.0)) + 1\n",
    "    if 1 <= cell_x <= 600 and 1 <= cell_y <= 600:\n",
    "        return [cell_x, cell_y]\n",
    "    return None\n",
    "\n",
    "udf_get_250m_cell = udf(get_250m_cell, ArrayType(IntegerType()))\n",
    "\n",
    "df_cells = (\n",
    "    df\n",
    "    # Add columns for start_cell_x, start_cell_y, end_cell_x, end_cell_y\n",
    "    .withColumn(\"start_cell\",\n",
    "        udf_get_250m_cell(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"start_cell_x\", expr(\"start_cell[0]\"))\n",
    "    .withColumn(\"start_cell_y\", expr(\"start_cell[1]\"))\n",
    "    .withColumn(\"end_cell\",\n",
    "        udf_get_250m_cell(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .withColumn(\"end_cell_x\", expr(\"end_cell[0]\"))\n",
    "    .withColumn(\"end_cell_y\", expr(\"end_cell[1]\"))\n",
    "    # Keep only rows with valid cells\n",
    "    .filter(col(\"start_cell_x\").isNotNull() & col(\"end_cell_x\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa63633b-f72a-4ec0-8b63-2f89496d1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP B: Profit Aggregation (last 15 min)\n",
    "########################################\n",
    "# Windowed aggregator on dropoff_datetime\n",
    "# Median fare+tip for trips that ended in last 15 min, grouped by pickup cell.\n",
    "from pyspark.sql.functions import percentile_approx, window\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "profit_agg = (\n",
    "    df_cells\n",
    "    .withColumn(\"fare_plus_tip\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    .withWatermark(\"dropoff_datetime\", \"20 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"dropoff_datetime\", \"15 minutes\"),\n",
    "        col(\"start_cell_x\"),\n",
    "        col(\"start_cell_y\")\n",
    "    )\n",
    "    .agg(\n",
    "        percentile_approx(\"fare_plus_tip\", 0.5).alias(\"median_fare_tip\"),\n",
    "        spark_max(\"pickup_datetime\").alias(\"trigger_pickup\"),\n",
    "        spark_max(\"dropoff_datetime\").alias(\"trigger_dropoff\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c2da62-8494-46be-974e-59b067ae1c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:43:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d63ef4d8-21fd-42ed-921a-cc04808368d1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/30 17:43:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/30 17:43:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-01bd86f7-ac72-467e-9ae2-80939a30e821. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/30 17:43:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/03/30 17:44:07 ERROR Utils: Uncaught exception in thread task-result-getter-2\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7008/0x0000000842200840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1484)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1107)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1456)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1085)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7009/0x00000008420f7440.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6982/0x0000000842245840.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6991/0x00000008421b0040.apply(Unknown Source)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$4018/0x000000084173fc40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "Exception in thread \"task-result-getter-2\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7008/0x0000000842200840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1484)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1107)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1456)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1085)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7009/0x00000008420f7440.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6982/0x0000000842245840.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6991/0x00000008421b0040.apply(Unknown Source)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$4018/0x000000084173fc40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "25/03/30 17:44:08 ERROR Utils: Uncaught exception in thread task-result-getter-0\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7008/0x0000000842200840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1484)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1107)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1456)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1085)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7009/0x00000008420f7440.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6982/0x0000000842245840.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6991/0x00000008421b0040.apply(Unknown Source)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$4018/0x000000084173fc40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "Exception in thread \"task-result-getter-0\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$1$adapted(ChunkedByteBuffer.scala:243)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7008/0x0000000842200840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1484)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1107)\n",
      "\tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1456)\n",
      "\tat org.apache.commons.io.IOUtils.copy(IOUtils.java:1085)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.$anonfun$fromFile$2(ChunkedByteBuffer.scala:245)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$$Lambda$7009/0x00000008420f7440.apply$mcI$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromFile(ChunkedByteBuffer.scala:246)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$.fromManagedBuffer(ChunkedByteBuffer.scala:220)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBytes$1(BlockManager.scala:1263)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6982/0x0000000842245840.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6991/0x00000008421b0040.apply(Unknown Source)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3$$Lambda$4018/0x000000084173fc40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "25/03/30 17:44:08 ERROR Utils: Uncaught exception in thread task-result-getter-3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"task-result-getter-3\" java.lang.OutOfMemoryError: Java heap space\n",
      "[Stage 38:======================================================> (96 + 3) / 99]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 10 Profitable Areas (batch=0) ===\n",
      "{'pickup_datetime': '2013-01-01 02:03:00', 'dropoff_datetime': '2013-01-01 02:08:00', 'profitable_cell_id_1': '309.334', 'empty_taxies_in_cell_id_1': 0, 'median_profit_in_cell_id_1': 8.25, 'profitability_of_cell_1': None, 'profitable_cell_id_2': '327.342', 'empty_taxies_in_cell_id_2': 0, 'median_profit_in_cell_id_2': 5.0, 'profitability_of_cell_2': None, 'profitable_cell_id_3': '310.333', 'empty_taxies_in_cell_id_3': 0, 'median_profit_in_cell_id_3': 9.1, 'profitability_of_cell_3': None, 'profitable_cell_id_4': '317.301', 'empty_taxies_in_cell_id_4': 0, 'median_profit_in_cell_id_4': 9.5, 'profitability_of_cell_4': None, 'profitable_cell_id_5': '317.316', 'empty_taxies_in_cell_id_5': 0, 'median_profit_in_cell_id_5': 4.5, 'profitability_of_cell_5': None, 'profitable_cell_id_6': '307.320', 'empty_taxies_in_cell_id_6': 0, 'median_profit_in_cell_id_6': 9.5, 'profitability_of_cell_6': None, 'profitable_cell_id_7': '309.331', 'empty_taxies_in_cell_id_7': 0, 'median_profit_in_cell_id_7': 7.5, 'profitability_of_cell_7': None, 'profitable_cell_id_8': '314.324', 'empty_taxies_in_cell_id_8': 0, 'median_profit_in_cell_id_8': 10.0, 'profitability_of_cell_8': None, 'profitable_cell_id_9': '309.330', 'empty_taxies_in_cell_id_9': 0, 'median_profit_in_cell_id_9': 12.0, 'profitability_of_cell_9': None, 'profitable_cell_id_10': '311.336', 'empty_taxies_in_cell_id_10': 0, 'median_profit_in_cell_id_10': 11.5, 'profitability_of_cell_10': None, 'delay': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:45:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 139107 milliseconds\n",
      "[Stage 38:======================================================> (96 + 3) / 99]\r"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 3. Track \"empty\" taxis with ephemeral dictionary in foreachBatch\n",
    "########################################\n",
    "# We will store ephemeral state in a global dictionary:\n",
    "# empty_state[medallion] = (dropoff_time, end_cell_x, end_cell_y)\n",
    "# If we see a subsequent pickup after dropoff_time, remove it.\n",
    "# We'll mark a taxi as empty if dropoff_time < 30 min ago.\n",
    "\n",
    "empty_state = {}  # global dictionary\n",
    "last_top10 = None  # track previous top 10\n",
    "\n",
    "########################################\n",
    "# 3.1. We'll define a microbatch function that:\n",
    "#   - collects the new rows\n",
    "#   - updates empty_state for each row\n",
    "#   - for each taxi, if dropoff_time < 30 min ago and no new pickup -> that taxi is empty\n",
    "#   - produce a (cell_x, cell_y, 1) row for each empty taxi\n",
    "#   - store them in a local aggregator: empty_count_map[(cell_x, cell_y)] = number_of_empty\n",
    "########################################\n",
    "\n",
    "def process_empty_taxis(batch_df, batch_id):\n",
    "    global empty_state\n",
    "    rows = batch_df.collect()  # gather micro-batch\n",
    "    if not rows:\n",
    "        return\n",
    "    \n",
    "    # Update ephemeral dictionary\n",
    "    now_ts = datetime.datetime.utcnow()\n",
    "    \n",
    "    for r in rows:\n",
    "        med = r[\"medallion\"]\n",
    "        pick_t = r[\"pickup_datetime\"]\n",
    "        drop_t = r[\"dropoff_datetime\"]\n",
    "        end_x = r[\"end_cell_x\"]\n",
    "        end_y = r[\"end_cell_y\"]\n",
    "        \n",
    "        # update dropoff\n",
    "        if drop_t:\n",
    "            empty_state[med] = (drop_t, end_x, end_y)\n",
    "        \n",
    "        # if there's a subsequent pickup after the last dropoff, remove it\n",
    "        old_val = empty_state.get(med, None)\n",
    "        if old_val and pick_t:\n",
    "            (old_drop_t, old_cellx, old_celly) = old_val\n",
    "            if pick_t > old_drop_t:\n",
    "                # not empty\n",
    "                del empty_state[med]\n",
    "\n",
    "# We'll write df_cells to a sink with foreachBatch=process_empty_taxis\n",
    "# But that won't produce final output. We'll store empty results in ephemeral dict.\n",
    "# Then next step is to gather \"current empties\" for a join with the profit aggregator in a separate foreachBatch.\n",
    "\n",
    "empty_query = (\n",
    "    df_cells\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # just to trigger micro-batches\n",
    "    .format(\"console\")     # or any sink\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(process_empty_taxis)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "########################################\n",
    "# 4. Next, we combine ephemeral empty-taxi info with 15-min profit aggregator\n",
    "#    in a foreachBatch on the profit aggregator\n",
    "########################################\n",
    "\n",
    "def process_profit(batch_df, batch_id):\n",
    "    global empty_state, last_top10\n",
    "    \n",
    "    # This micro-batch has the windowed aggregator for median_fare_tip\n",
    "    # We'll build a local aggregator for empties: (cell_x, cell_y) -> count\n",
    "    now_ts = datetime.datetime.utcnow()\n",
    "    \n",
    "    # 4.1. Build an ephemeral aggregator for empties\n",
    "    empty_map = {}\n",
    "    for med, (drop_time, cx, cy) in empty_state.items():\n",
    "        # if drop_time < 30 min\n",
    "        if drop_time is not None:\n",
    "            delta_sec = (now_ts - drop_time).total_seconds()\n",
    "            if delta_sec <= 1800:\n",
    "                key = (cx, cy)\n",
    "                empty_map[key] = empty_map.get(key, 0) + 1\n",
    "    \n",
    "    # 4.2. Convert batch_df to a local list\n",
    "    profit_rows = batch_df.collect()\n",
    "    if not profit_rows:\n",
    "        return\n",
    "    \n",
    "    # We'll produce final joined rows: (cell_x, cell_y, median_fare_tip, empty_taxis, profitability, pickup, dropoff)\n",
    "    final_list = []\n",
    "    for pr in profit_rows:\n",
    "        sx = pr[\"start_cell_x\"]\n",
    "        sy = pr[\"start_cell_y\"]\n",
    "        medf = pr[\"median_fare_tip\"]\n",
    "        empties = empty_map.get((sx, sy), 0)\n",
    "        profitval = (medf / empties) if empties > 0 else None\n",
    "        final_list.append({\n",
    "            \"cell_x\": sx,\n",
    "            \"cell_y\": sy,\n",
    "            \"median_fare_tip\": medf,\n",
    "            \"empty_taxis\": empties,\n",
    "            \"profitability\": profitval,\n",
    "            \"pickup\": pr[\"trigger_pickup\"],\n",
    "            \"dropoff\": pr[\"trigger_dropoff\"]\n",
    "        })\n",
    "    \n",
    "    # 4.3. Sort descending by profitability, pick top 10\n",
    "    final_list.sort(key=lambda row: (row[\"profitability\"] if row[\"profitability\"] else 0), reverse=True)\n",
    "    top10 = final_list[:10]\n",
    "    \n",
    "    # 4.4. Compare with last_top10 (optional). For simplicity we always print here\n",
    "    if top10:\n",
    "        # Build a single line output\n",
    "        out = {}\n",
    "        out[\"pickup_datetime\"] = str(top10[0][\"pickup\"]) if top10[0][\"pickup\"] else None\n",
    "        out[\"dropoff_datetime\"] = str(top10[0][\"dropoff\"]) if top10[0][\"dropoff\"] else None\n",
    "        \n",
    "        for i in range(len(top10)):\n",
    "            idx = i+1\n",
    "            rowi = top10[i]\n",
    "            out[f\"profitable_cell_id_{idx}\"] = f\"{rowi['cell_x']}.{rowi['cell_y']}\"\n",
    "            out[f\"empty_taxies_in_cell_id_{idx}\"] = rowi[\"empty_taxis\"]\n",
    "            out[f\"median_profit_in_cell_id_{idx}\"] = rowi[\"median_fare_tip\"]\n",
    "            out[f\"profitability_of_cell_{idx}\"] = rowi[\"profitability\"]\n",
    "        \n",
    "        # Fill up to 10\n",
    "        for j in range(len(top10)+1, 11):\n",
    "            out[f\"profitable_cell_id_{j}\"] = None\n",
    "            out[f\"empty_taxies_in_cell_id_{j}\"] = None\n",
    "            out[f\"median_profit_in_cell_id_{j}\"] = None\n",
    "            out[f\"profitability_of_cell_{j}\"] = None\n",
    "        \n",
    "        out[\"delay\"] = 1.0  # placeholder\n",
    "        print(f\"\\n=== Top 10 Profitable Areas (batch={batch_id}) ===\")\n",
    "        print(out)\n",
    "\n",
    "# 4.5. Write the profit aggregator in a foreachBatch, calling process_profit\n",
    "profit_final_query = (\n",
    "    profit_agg\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"console\")     # or any sink\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(process_profit)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# profit_final_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca616b0-2499-473a-ac06-66249a155a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec06b6-bb5e-404b-8ba7-d3ed5334a507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d3f60-c9f6-4c7d-a7f8-39156324706c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72877ef9-fa83-457a-81c9-a21365764266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562712ae-496f-45d5-84d5-6353739ad847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbad974-acb7-40a4-bdb8-f3fe613cea69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ef877-86e1-48b5-8a6d-cc196dbab93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8de56b-91f9-44ea-a7ca-abdb171f3a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08a771-17df-46d1-af86-f5effd1f162d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3b8fb-6236-4c87-8af7-0ffba17c750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122ba92-5f02-492b-aa52-6ea30ea7faa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5f834-3585-4ed3-8440-b74a261e613a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54062559-50fd-47b3-ae06-baae61838eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c43b4-1e85-4354-a0e3-6651271b6e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8422ec-63f1-43c7-90bb-63129326160d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c4f0e-d987-4f20-91b6-56f22b9c4d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12abfb-c080-4201-adcb-2f7f9d02b5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f01f7-1422-4498-a240-031ac1469ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fc27d-4b8f-4bc9-87f9-de22d5924127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac05243-1e20-47eb-9242-548ef6869d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9183f-8430-4c14-be79-d98d2b723224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8096d-7987-4122-892a-f58be47bc796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726a3f0-e25c-4d7a-9eec-c7f963a4d48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377dee5-6a9f-447d-9c48-7d1bc097c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d77901-1863-40c2-a399-2436329349a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57de1a5-87d9-433b-9ee9-11b37ebe5a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf40b17-64bd-4805-8f6c-e73f48de9d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3664ca-7724-4303-931b-cfdc065a86fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa090c-189c-48d3-a1b1-935ff8a52ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674c362f-2bc8-4057-a527-f2a9e7c20aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: 250m x 250m Cell Mapping\n",
    "#######################################\n",
    "LAT_REF = 41.474937\n",
    "LON_REF = -74.913585\n",
    "EARTH_RADIUS = 6371000.0\n",
    "\n",
    "def latlon_to_meters(lat, lon):\n",
    "    lat_r = math.radians(lat)\n",
    "    lon_r = math.radians(lon)\n",
    "    lat_ref_r = math.radians(LAT_REF)\n",
    "    lon_ref_r = math.radians(LON_REF)\n",
    "    x = EARTH_RADIUS * (lon_r - lon_ref_r) * math.cos((lat_r + lat_ref_r)/2)\n",
    "    y = EARTH_RADIUS * (lat_r - lat_ref_r)\n",
    "    return (x, y)\n",
    "\n",
    "def get_250m_cell(lat, lon):\n",
    "    (x_m, y_m) = latlon_to_meters(lat, lon)\n",
    "    cell_x = int(math.floor(x_m / 250.0)) + 1\n",
    "    cell_y = int(math.floor((-1 * y_m) / 250.0)) + 1\n",
    "    if 1 <= cell_x <= 600 and 1 <= cell_y <= 600:\n",
    "        return [cell_x, cell_y]\n",
    "    return None\n",
    "\n",
    "@udf(ArrayType(IntegerType()))\n",
    "def udf_get_250m_cell(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    return get_250m_cell(lat, lon)\n",
    "\n",
    "df_cells = (df\n",
    "    .withColumn(\"start_cell\", udf_get_250m_cell(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"end_cell\", udf_get_250m_cell(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .withColumn(\"start_cell_x\", expr(\"start_cell[0]\"))\n",
    "    .withColumn(\"start_cell_y\", expr(\"start_cell[1]\"))\n",
    "    .withColumn(\"end_cell_x\", expr(\"end_cell[0]\"))\n",
    "    .withColumn(\"end_cell_y\", expr(\"end_cell[1]\"))\n",
    "    .filter(col(\"start_cell_x\").isNotNull() & col(\"end_cell_x\").isNotNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539ace23-f9d9-464e-a091-f002d3952a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Ephemeral Python State for Empty Taxi Tracking and Profit Aggregation\n",
    "#######################################\n",
    "# Global dictionary to track for each taxi its last dropoff state:\n",
    "dropoff_state = {}  # key: medallion, value: (dropoff_datetime, end_cell_x, end_cell_y)\n",
    "last_top10 = None   # to track previous top 10 result\n",
    "\n",
    "def median(values):\n",
    "    if not values:\n",
    "        return None\n",
    "    s = sorted(values)\n",
    "    n = len(s)\n",
    "    mid = n // 2\n",
    "    return s[mid] if n % 2 == 1 else (s[mid-1] + s[mid]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a7134a7-2b48-452c-bf46-ffd9ee67a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 16:58:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2455a1c5-75c0-4a2c-a815-c2090470edbc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/30 16:58:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 32:(10 + 12) / 100][Stage 34:>  (0 + 0) / 50][Stage 35:>  (0 + 0) / 50]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percentile_approx, window, desc, max as spark_max, approx_count_distinct\n",
    "\n",
    "# Step 1: Add fare_plus_tip column\n",
    "with_fare = df_cells.withColumn(\"fare_plus_tip\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "\n",
    "# Step 2: Profit Aggregation: Median fare+tip per 15-minute window grouped by start_cell\n",
    "profit_df = (\n",
    "    with_fare\n",
    "    .withWatermark(\"dropoff_datetime\", \"20 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"dropoff_datetime\", \"15 minutes\"),\n",
    "        col(\"start_cell_x\"), col(\"start_cell_y\")\n",
    "    )\n",
    "    .agg(\n",
    "        percentile_approx(\"fare_plus_tip\", 0.5).alias(\"median_fare_tip\"),\n",
    "        spark_max(\"pickup_datetime\").alias(\"trigger_pickup\"),\n",
    "        spark_max(\"dropoff_datetime\").alias(\"trigger_dropoff\")\n",
    "    )\n",
    ")\n",
    "\n",
    "profit_query = (\n",
    "    profit_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")          # Or \"update\", depending on your aggregation logic\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab60ce7-d760-4746-90cb-37fae8efcd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[window: struct<start:timestamp,end:timestamp>, cell_x: int, cell_y: int, empty_taxis: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:(14 + 12) / 100][Stage 34:>  (0 + 0) / 50][Stage 35:>  (0 + 0) / 50]\r"
     ]
    }
   ],
   "source": [
    "# Step 3: Empty Taxi Aggregation: Count distinct medallions in end_cell per 30-minute window\n",
    "empty_df = (\n",
    "    df_cells\n",
    "    .withWatermark(\"dropoff_datetime\", \"35 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"dropoff_datetime\", \"30 minutes\"),\n",
    "        col(\"end_cell_x\").alias(\"cell_x\"),\n",
    "        col(\"end_cell_y\").alias(\"cell_y\")\n",
    "    )\n",
    "    .agg(\n",
    "        approx_count_distinct(\"medallion\").alias(\"empty_taxis\")\n",
    "    )\n",
    ")\n",
    "empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367f09f2-9257-48b2-bac8-346f90e96881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trigger_pickup: timestamp, trigger_dropoff: timestamp, start_cell_x: int, start_cell_y: int, median_fare_tip: double, empty_taxis: bigint, profitability: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Join the two aggregations on matching cell coordinates\n",
    "joined_df = (\n",
    "    profit_df.join(\n",
    "        empty_df,\n",
    "        (profit_df.start_cell_x == empty_df.cell_x) &\n",
    "        (profit_df.start_cell_y == empty_df.cell_y),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        profit_df.trigger_pickup,\n",
    "        profit_df.trigger_dropoff,\n",
    "        profit_df.start_cell_x,\n",
    "        profit_df.start_cell_y,\n",
    "        profit_df.median_fare_tip,\n",
    "        empty_df.empty_taxis\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"profitability\",\n",
    "        when(col(\"empty_taxis\") > 0, col(\"median_fare_tip\") / col(\"empty_taxis\"))\n",
    "    )\n",
    ")\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c854be-61b1-4f8e-a5d5-aa9f218dc53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 16:35:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-764bde61-479c-47ac-92c2-e7405dfdab0e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/30 16:35:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 10 CHANGED (batch 0) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                      (0 + 12) / 100]\r"
     ]
    }
   ],
   "source": [
    "# Step 5: Output top 10 profitable cells per batch\n",
    "last_top10 = None\n",
    "\n",
    "def print_top_10(batch_df, batch_id):\n",
    "    global last_top10\n",
    "    top10 = batch_df.orderBy(desc(\"profitability\")).limit(10).collect()\n",
    "    if top10 != last_top10:\n",
    "        print(f\"\\n=== TOP 10 CHANGED (batch {batch_id}) ===\")\n",
    "        for i, row in enumerate(top10, start=1):\n",
    "            print(f\"{i:02d}. Cell {row['start_cell_x']}.{row['start_cell_y']} → Profitability: {row['profitability']:.4f} | Median Profit: {row['median_fare_tip']:.2f} | Empty Taxis: {row['empty_taxis']}\")\n",
    "        last_top10 = top10\n",
    "\n",
    "# Start streaming query\n",
    "query = (\n",
    "    joined_df\n",
    "    .writeStream\n",
    "    .foreachBatch(print_top_10)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e72cc6-6d42-44f5-a7a3-883405c23ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query name: None\n",
      "Status: {'message': 'No new data but cleaning up state', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "Is active: True\n",
      "Query name: None\n",
      "Status: {'message': 'Getting offsets from DeltaSource[file:/gpfs/helios/home/fidankarimova/input/spark-warehouse/clean_taxi_data]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "Is active: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=>            (10 + 12) / 100][Stage 34:>                (0 + 0) / 50]\r"
     ]
    }
   ],
   "source": [
    "for query in spark.streams.active:\n",
    "    print(f\"Query name: {query.name}\")\n",
    "    print(f\"Status: {query.status}\")\n",
    "    print(f\"Is active: {query.isActive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae7a75-295b-48b4-81c9-7ebcf8b90616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef96144-b1a6-439d-b114-575d527efedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d4559-4ab0-489c-a610-51f1c27f4832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f88e4e-c1cc-4bec-8c75-920bd48f23ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f21de2-7790-4a86-84f6-ddd818c9f2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa02254-069c-4a84-b8f3-1f20463738d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b23a3-36d9-4b26-ac6c-cd8e250ff883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
