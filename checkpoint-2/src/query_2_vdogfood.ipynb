{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c24fba-3e17-439b-a923-e936af4209fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Part 0: Imports and Spark Session Setup\n",
    "#######################################\n",
    "import math, time, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, expr, when, lit, array, percentile_approx, desc\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType,\n",
    "    DoubleType, IntegerType, ArrayType\n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, window, unix_timestamp, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, to_json, struct, lit, current_timestamp, expr, when, from_json, window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7c2b26-acb4-4120-aa34-c82a2c08cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse directory: file:/home/jovyan/spark-warehouse\n",
      "Catalog implementation: hive\n"
     ]
    }
   ],
   "source": [
    "WAREHOUSE_DIR = \"/home/jovyan/spark-warehouse\"\n",
    "\n",
    "def create_spark_session(app_name=\"FrequentRoutes\"):\n",
    "    \"\"\"\n",
    "    start spark session with kafka and delta support / memory config setup too\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder.appName(app_name) \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", WAREHOUSE_DIR) \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.driver.memory\", \"5g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.default.parallelism\", \"100\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", 100) \\\n",
    "        .enableHiveSupport()\n",
    "    \n",
    "    # delta config\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    \n",
    "    # do not flood logs\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    # Print configs for debugging\n",
    "    print(f\"Warehouse directory: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "    print(f\"Catalog implementation: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5903f28-6d03-4fba-bd56-9613b36c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Part 1: Define Schema and Read Cleaned Data as a Stream\n",
    "#######################################\n",
    "def create_raw_taxi_schema():\n",
    "    return StructType([\n",
    "        StructField(\"medallion\", StringType(), True),\n",
    "        StructField(\"hack_license\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "        StructField(\"payment_type\", StringType(), True),\n",
    "        StructField(\"fare_amount\", DoubleType(), True),\n",
    "        StructField(\"surcharge\", DoubleType(), True),\n",
    "        StructField(\"mta_tax\", DoubleType(), True),\n",
    "        StructField(\"tip_amount\", DoubleType(), True),\n",
    "        StructField(\"tolls_amount\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "raw_schema = create_raw_taxi_schema()\n",
    "cleaned_parquet_path = \"cleaned_data.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a80ed-e049-445d-b59e-2482ecf43c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f81b634-6872-48d9-8b68-30d8883e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet files as a stream; we simulate streaming with maxFilesPerTrigger.\n",
    "df_raw = (spark.readStream\n",
    "    .schema(raw_schema)\n",
    "    .format(\"parquet\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(cleaned_parquet_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83824b8-7004-49e9-868a-299b0d8c57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Part 2: 250m x 250m Cell Mapping\n",
    "#######################################\n",
    "LAT_REF = 41.474937\n",
    "LON_REF = -74.913585\n",
    "EARTH_RADIUS = 6371000.0\n",
    "\n",
    "def latlon_to_meters(lat, lon):\n",
    "    lat_r = math.radians(lat)\n",
    "    lon_r = math.radians(lon)\n",
    "    lat_ref_r = math.radians(LAT_REF)\n",
    "    lon_ref_r = math.radians(LON_REF)\n",
    "    x = EARTH_RADIUS * (lon_r - lon_ref_r) * math.cos((lat_r + lat_ref_r)/2)\n",
    "    y = EARTH_RADIUS * (lat_r - lat_ref_r)\n",
    "    return (x, y)\n",
    "\n",
    "def get_250m_cell(lat, lon):\n",
    "    (x_m, y_m) = latlon_to_meters(lat, lon)\n",
    "    cell_x = int(math.floor(x_m / 250.0)) + 1\n",
    "    cell_y = int(math.floor((-1 * y_m) / 250.0)) + 1\n",
    "    if 1 <= cell_x <= 600 and 1 <= cell_y <= 600:\n",
    "        return [cell_x, cell_y]\n",
    "    return None\n",
    "\n",
    "@udf(ArrayType(IntegerType()))\n",
    "def udf_get_250m_cell(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    return get_250m_cell(lat, lon)\n",
    "\n",
    "df_cells = (df_raw\n",
    "    .withColumn(\"start_cell\", udf_get_250m_cell(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"end_cell\", udf_get_250m_cell(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .withColumn(\"start_cell_x\", expr(\"start_cell[0]\"))\n",
    "    .withColumn(\"start_cell_y\", expr(\"start_cell[1]\"))\n",
    "    .withColumn(\"end_cell_x\", expr(\"end_cell[0]\"))\n",
    "    .withColumn(\"end_cell_y\", expr(\"end_cell[1]\"))\n",
    "    .filter(col(\"start_cell_x\").isNotNull() & col(\"end_cell_x\").isNotNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197db88c-09eb-43f1-8819-d9b9e2d6345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Part 3: Ephemeral Python State for Empty Taxi Tracking and Profit Aggregation\n",
    "#######################################\n",
    "# Global dictionary to track for each taxi its last dropoff state:\n",
    "dropoff_state = {}  # key: medallion, value: (dropoff_datetime, end_cell_x, end_cell_y)\n",
    "last_top10 = None   # to track previous top 10 result\n",
    "\n",
    "def median(values):\n",
    "    if not values:\n",
    "        return None\n",
    "    s = sorted(values)\n",
    "    n = len(s)\n",
    "    mid = n // 2\n",
    "    return s[mid] if n % 2 == 1 else (s[mid-1] + s[mid]) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1e6859d-8b97-4780-94f3-66a3b9b9cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_microbatch(batch_df, batch_id):\n",
    "    global dropoff_state, last_top10\n",
    "\n",
    "    read_time = time.time()\n",
    "\n",
    "    \n",
    "    # We'll do row-by-row iteration using toLocalIterator\n",
    "    now_ts = datetime.datetime.utcnow()\n",
    "    cutoff_15 = now_ts - datetime.timedelta(minutes=15)\n",
    "    cutoff_30 = now_ts - datetime.timedelta(minutes=30)\n",
    "    \n",
    "    # We'll store partial results in:\n",
    "    # 1) profit_map: (start_cell_x, start_cell_y) -> list of fare+tip for trips ended < 15 min\n",
    "    profit_map = {}\n",
    "    \n",
    "    # Track max dropoff time in this microbatch for \"triggering\" event:\n",
    "    max_dropoff_t = None\n",
    "    max_pickup_t_for_that = None\n",
    "    \n",
    "    # Row-by-row iteration\n",
    "    for row in batch_df.toLocalIterator():\n",
    "        medallion = row[\"medallion\"]\n",
    "        pickup_t = row[\"pickup_datetime\"]\n",
    "        dropoff_t = row[\"dropoff_datetime\"]\n",
    "        start_x = row[\"start_cell_x\"]\n",
    "        start_y = row[\"start_cell_y\"]\n",
    "        end_x = row[\"end_cell_x\"]\n",
    "        end_y = row[\"end_cell_y\"]\n",
    "        fare = row[\"fare_amount\"] or 0.0\n",
    "        tip = row[\"tip_amount\"] or 0.0\n",
    "        fare_plus_tip = fare + tip\n",
    "        \n",
    "        # Update \"triggering\" event times\n",
    "        if dropoff_t and (max_dropoff_t is None or dropoff_t > max_dropoff_t):\n",
    "            max_dropoff_t = dropoff_t\n",
    "            max_pickup_t_for_that = pickup_t\n",
    "        \n",
    "        # 1) Update dropoff_state\n",
    "        old_val = dropoff_state.get(medallion, None)\n",
    "        \n",
    "        # If there's a new dropoff, store it\n",
    "        if dropoff_t is not None:\n",
    "            dropoff_state[medallion] = (dropoff_t, end_x, end_y)\n",
    "        \n",
    "        # If there's a pickup that is after the last dropoff, taxi is no longer empty\n",
    "        if pickup_t and old_val:\n",
    "            (old_drop_time, old_cell_x, old_cell_y) = old_val\n",
    "            if pickup_t > old_drop_time:\n",
    "                # remove from dropoff_state\n",
    "                dropoff_state[medallion] = None\n",
    "                if medallion in dropoff_state:\n",
    "                    del dropoff_state[medallion]\n",
    "        \n",
    "        # 2) For the 15-min median, if dropoff_t <= now & dropoff_t >= cutoff_15\n",
    "        if dropoff_t:\n",
    "            if dropoff_t >= cutoff_15:\n",
    "                k = (start_x, start_y)\n",
    "                profit_map.setdefault(k, []).append(fare_plus_tip)\n",
    "    \n",
    "    # Now build empty_count_map from the ephemeral dropoff_state\n",
    "    empty_count_map = {}\n",
    "    for med, val in dropoff_state.items():\n",
    "        if val is None:\n",
    "            continue\n",
    "        (d_time, cellx, celly) = val\n",
    "        if d_time >= cutoff_30:\n",
    "            # still within 30 min\n",
    "            k = (cellx, celly)\n",
    "            empty_count_map[k] = empty_count_map.get(k, 0) + 1\n",
    "    \n",
    "    # Build a profit_result map from profit_map\n",
    "    #  => (cell_x, cell_y) -> median_fare_tip\n",
    "    profit_result = {}\n",
    "    for (cx, cy), fares in profit_map.items():\n",
    "        mm = median(fares)\n",
    "        profit_result[(cx, cy)] = mm\n",
    "    \n",
    "    # Build final list: (cell_x, cell_y, empties, median_fare_tip, profitability)\n",
    "    results = []\n",
    "    for (cx, cy), med_val in profit_result.items():\n",
    "        empties = empty_count_map.get((cx, cy), 0)\n",
    "        if empties > 0 and med_val is not None:\n",
    "            profit_val = med_val / empties\n",
    "        else:\n",
    "            profit_val = None\n",
    "        results.append((cx, cy, empties, med_val, profit_val))\n",
    "    \n",
    "    # Sort descending by profitability\n",
    "    results.sort(key=lambda x: (x[4] if x[4] is not None else 0), reverse=True)\n",
    "    top10 = results[:10]\n",
    "    \n",
    "    # Compare with last_top10\n",
    "    has_changed = False\n",
    "    if not last_top10 and top10:\n",
    "        has_changed = True\n",
    "    elif last_top10 and len(top10) != len(last_top10):\n",
    "        has_changed = True\n",
    "    else:\n",
    "        for i in range(len(top10)):\n",
    "            if (top10[i][0] != last_top10[i][0] or\n",
    "                top10[i][1] != last_top10[i][1]):\n",
    "                has_changed = True\n",
    "                break\n",
    "    \n",
    "    if has_changed and top10:\n",
    "        # Build the single-row output\n",
    "        # \"pickup_datetime, dropoff_datetime\" from the \"triggering\" trip\n",
    "        trigger_pickup = max_pickup_t_for_that\n",
    "        trigger_dropoff = max_dropoff_t\n",
    "        \n",
    "        output_row = {\n",
    "            \"pickup_datetime\": str(trigger_pickup) if trigger_pickup else None,\n",
    "            \"dropoff_datetime\": str(trigger_dropoff) if trigger_dropoff else None\n",
    "        }\n",
    "        \n",
    "        # fill columns for top10\n",
    "        for i in range(10):\n",
    "            idx = i+1\n",
    "            if i < len(top10):\n",
    "                (cx, cy, ecount, medp, prof) = top10[i]\n",
    "                output_row[f\"profitable_cell_id_{idx}\"] = f\"{cx}.{cy}\"\n",
    "                output_row[f\"empty_taxies_in_cell_id_{idx}\"] = ecount\n",
    "                output_row[f\"median_profit_in_cell_id_{idx}\"] = medp\n",
    "                output_row[f\"profitability_of_cell_{idx}\"] = prof\n",
    "            else:\n",
    "                output_row[f\"profitable_cell_id_{idx}\"] = None\n",
    "                output_row[f\"empty_taxies_in_cell_id_{idx}\"] = None\n",
    "                output_row[f\"median_profit_in_cell_id_{idx}\"] = None\n",
    "                output_row[f\"profitability_of_cell_{idx}\"] = None\n",
    "        \n",
    "        # # 'delay' = (now - hypothetical read_time). We'll just put a placeholder\n",
    "        # delay_val = 1.0\n",
    "        # output_row[\"delay\"] = delay_val\n",
    "\n",
    "        # Actual delay measurement: difference between output_time and read_time\n",
    "        output_time = time.time()\n",
    "        delay_val = output_time - read_time\n",
    "        output_row[\"delay\"] = delay_val\n",
    "        \n",
    "        print(\"\\n=== TOP 10 CHANGED === (batch_id = {})\".format(batch_id))\n",
    "        print(output_row)\n",
    "        \n",
    "        last_top10 = top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee1b69-03f3-4da4-8d28-17b00b6f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Part 5: Launch the Streaming Query\n",
    "#######################################\n",
    "query = (df_cells\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # We simply need micro-batch triggers\n",
    "    .format(\"console\")     # Using console sink for demonstration\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(process_microbatch)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aec3976-0fce-4f58-87a2-c6e89df23d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 14:46:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f8a218ff-2ce9-43fc-94fb-3368a415f0b4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/29 14:46:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion|hack_license|pickup_datetime|dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 14:46:23 ERROR Executor: Exception in task 11.0 in stage 1.0 (TID 12)4]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2868/0x00000008411b7840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1873)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1782)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1451)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1369)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/29 14:46:23 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 11.0 in stage 1.0 (TID 12),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2868/0x00000008411b7840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1873)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1782)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1451)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1369)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/29 14:46:23 WARN TaskSetManager: Lost task 11.0 in stage 1.0 (TID 12) (pegasus2.hpc.ut.ee executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2868/0x00000008411b7840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1873)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1782)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1451)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1369)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1543)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1500)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1423)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/03/29 14:46:23 ERROR TaskSetManager: Task 11 in stage 1.0 failed 1 times; aborting job\n",
      "25/03/29 14:46:23 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/03/29 14:46:23 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/gpfs/helios/home/fidankarimova/input/pythonenv/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o83.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m test_query \u001b[38;5;241m=\u001b[39m (df_raw\u001b[38;5;241m.\u001b[39mwriteStream\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mstart())\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtest_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_query\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/input/pythonenv/lib/python3.9/site-packages/pyspark/sql/streaming/query.py:219\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    216\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/input/pythonenv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/input/pythonenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/input/pythonenv/lib/python3.9/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o83.awaitTermination"
     ]
    }
   ],
   "source": [
    "test_query = (df_raw.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .start())\n",
    "test_query.awaitTermination(30)\n",
    "test_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08805ad-602c-44cd-bc15-a38b28034054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec645bb-6cfd-4138-91d0-a4a412d94fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e708414-b7f7-436f-9446-583cfebbbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0368146-d728-4abc-bc37-fecab514fb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e88df-b2d1-4973-ac78-52fef3286a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12ca31-c8a4-4c4c-bf0e-597ff3350281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51321d-77f7-4798-82a4-d2ccde4f69b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e2778-c49b-4f3a-935c-a3aebf42b16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cde513-111e-49ad-addc-c96372f72e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ec2ed-265f-40f3-a69e-7ad214980660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d1fb6-6a07-465d-b237-c30af6ae7813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdc08d-1a5f-48e5-95e4-1e30b9bd3950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5570fee-3832-45e9-8485-6bb5d7ccb994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756d5f4-ad0d-48e2-98a5-546a0da5e2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780237b8-e1a4-496d-b47d-df7a28e9e3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a450e5-2744-48b5-bc35-bd421e2fe5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3b03f-211e-4c9e-a89c-4bacf4004ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95d9fe1-ca33-4df8-8079-ec348f3b338c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_raw_taxi_schema():\n",
    "    return StructType([\n",
    "        StructField(\"medallion\", StringType(), True),\n",
    "        StructField(\"hack_license\", StringType(), True),\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "        StructField(\"payment_type\", StringType(), True),\n",
    "        StructField(\"fare_amount\", DoubleType(), True),\n",
    "        StructField(\"surcharge\", DoubleType(), True),\n",
    "        StructField(\"mta_tax\", DoubleType(), True),\n",
    "        StructField(\"tip_amount\", DoubleType(), True),\n",
    "        StructField(\"tolls_amount\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8a303d-243a-44c0-9c51-7dc0fd56a5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'option'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m static_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(TEST_OUTPUT_PATH)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Now, read the Delta table as a stream.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Note: We use format(\"delta\") here.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_OUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxFilesPerTrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'option'"
     ]
    }
   ],
   "source": [
    "raw_schema = create_raw_taxi_schema()\n",
    "\n",
    "# Define paths\n",
    "PARQUET_PATH = \"cleaned_data.parquet\"        # Path to your static cleaned parquet file\n",
    "TEST_OUTPUT_PATH = \"test_delta_table\"       # Output path for the Delta table\n",
    "\n",
    "# Read the cleaned Parquet file (batch mode)\n",
    "static_df = spark.read.schema(raw_schema).parquet(PARQUET_PATH)\n",
    "\n",
    "# Write the static data to a Delta table (batch mode)\n",
    "static_df.write.format(\"delta\").mode(\"overwrite\").save(TEST_OUTPUT_PATH)\n",
    "\n",
    "# Now, read the Delta table as a stream.\n",
    "# Note: We use format(\"delta\") here.\n",
    "df_raw = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(TEST_OUTPUT_PATH)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be5e066-7ba5-4618-9a90-65865b116558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_parquet_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the cleaned data as a stream (simulate live streaming with maxFilesPerTrigger).\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m (spark\u001b[38;5;241m.\u001b[39mreadStream\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mschema(raw_schema)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxFilesPerTrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;241m.\u001b[39mload(\u001b[43mcleaned_parquet_path\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Uncomment the following line to process only a small fraction for testing:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# .filter(\"rand() < 0.01\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Part 2: 250m x 250m Cell Mapping\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Grid reference point: (41.474937, -74.913585); each cell is 250m x 250m.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m LAT_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m41.474937\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_parquet_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the cleaned data as a stream (simulate live streaming with maxFilesPerTrigger).\n",
    "df_raw = (spark.readStream\n",
    "    .schema(raw_schema)\n",
    "    .format(\"parquet\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(cleaned_parquet_path)\n",
    "    # Uncomment the following line to process only a small fraction for testing:\n",
    "    # .filter(\"rand() < 0.01\")\n",
    ")\n",
    "\n",
    "#######################################\n",
    "# Part 2: 250m x 250m Cell Mapping\n",
    "#######################################\n",
    "# Grid reference point: (41.474937, -74.913585); each cell is 250m x 250m.\n",
    "LAT_REF = 41.474937\n",
    "LON_REF = -74.913585\n",
    "EARTH_RADIUS = 6371000.0  # in meters\n",
    "\n",
    "def latlon_to_meters(lat, lon):\n",
    "    lat_r = math.radians(lat)\n",
    "    lon_r = math.radians(lon)\n",
    "    lat_ref_r = math.radians(LAT_REF)\n",
    "    lon_ref_r = math.radians(LON_REF)\n",
    "    x = EARTH_RADIUS * (lon_r - lon_ref_r) * math.cos((lat_r + lat_ref_r) / 2)\n",
    "    y = EARTH_RADIUS * (lat_r - lat_ref_r)\n",
    "    return (x, y)\n",
    "\n",
    "def get_250m_cell(lat, lon):\n",
    "    (x_m, y_m) = latlon_to_meters(lat, lon)\n",
    "    cell_x = int(math.floor(x_m / 250.0)) + 1\n",
    "    cell_y = int(math.floor((-1 * y_m) / 250.0)) + 1\n",
    "    if 1 <= cell_x <= 600 and 1 <= cell_y <= 600:\n",
    "        return [cell_x, cell_y]\n",
    "    return None\n",
    "\n",
    "@udf(ArrayType(IntegerType()))\n",
    "def udf_get_250m_cell(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    return get_250m_cell(lat, lon)\n",
    "\n",
    "df_cells = (df_raw\n",
    "    .withColumn(\"start_cell\", udf_get_250m_cell(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"end_cell\", udf_get_250m_cell(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .withColumn(\"start_cell_x\", expr(\"start_cell[0]\"))\n",
    "    .withColumn(\"start_cell_y\", expr(\"start_cell[1]\"))\n",
    "    .withColumn(\"end_cell_x\", expr(\"end_cell[0]\"))\n",
    "    .withColumn(\"end_cell_y\", expr(\"end_cell[1]\"))\n",
    "    .filter(col(\"start_cell_x\").isNotNull() & col(\"end_cell_x\").isNotNull())\n",
    ")\n",
    "\n",
    "#######################################\n",
    "# Part 3: Windowed Aggregations for Profit and Empty Taxi Count\n",
    "#######################################\n",
    "# Aggregation A: Profit Aggregation\n",
    "# Compute the median of (fare_amount + tip_amount) for trips that ended in the last 15 minutes,\n",
    "# grouped by the start cell.\n",
    "df_profit = (df_cells\n",
    "    .withColumn(\"fare_plus_tip\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    .withWatermark(\"dropoff_datetime\", \"20 minutes\")\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"15 minutes\"),\n",
    "        col(\"start_cell_x\"), col(\"start_cell_y\")\n",
    "    )\n",
    "    .agg(\n",
    "        percentile_approx(\"fare_plus_tip\", 0.5).alias(\"median_fare_tip\"),\n",
    "        spark_max(\"pickup_datetime\").alias(\"trigger_pickup\"),\n",
    "        spark_max(\"dropoff_datetime\").alias(\"trigger_dropoff\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"profit_window_start\"),\n",
    "        col(\"window.end\").alias(\"profit_window_end\"),\n",
    "        col(\"start_cell_x\"),\n",
    "        col(\"start_cell_y\"),\n",
    "        col(\"median_fare_tip\"),\n",
    "        col(\"trigger_pickup\"),\n",
    "        col(\"trigger_dropoff\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "profit_query = (\n",
    "    df_profit\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")          # Or \"update\", depending on your aggregation logic\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Aggregation B: Empty Taxi Count\n",
    "# Use approx_count_distinct on medallion for dropoffs in the last 30 minutes,\n",
    "# grouped by the dropoff cell.\n",
    "df_empty = (df_cells\n",
    "    .withWatermark(\"dropoff_datetime\", \"35 minutes\")\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"end_cell_x\").alias(\"cell_x\"),\n",
    "        col(\"end_cell_y\").alias(\"cell_y\")\n",
    "    )\n",
    "    .agg(\n",
    "        approx_count_distinct(\"medallion\").alias(\"empty_taxis\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"empty_window_start\"),\n",
    "        col(\"window.end\").alias(\"empty_window_end\"),\n",
    "        col(\"cell_x\"),\n",
    "        col(\"cell_y\"),\n",
    "        col(\"empty_taxis\")\n",
    "    )\n",
    ")\n",
    "\n",
    "empty_query = (\n",
    "    df_empty\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")          # Or \"update\", depending on your aggregation logic\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Join the two aggregations on cell coordinates.\n",
    "df_joined = (df_profit.join(\n",
    "    df_empty,\n",
    "    (df_profit.start_cell_x == df_empty.cell_x) & (df_profit.start_cell_y == df_empty.cell_y),\n",
    "    \"inner\"\n",
    ")\n",
    ".select(\n",
    "    df_profit.trigger_pickup,\n",
    "    df_profit.trigger_dropoff,\n",
    "    df_profit.start_cell_x,\n",
    "    df_profit.start_cell_y,\n",
    "    df_profit.median_fare_tip,\n",
    "    df_empty.empty_taxis\n",
    ")\n",
    ".withColumn(\"profitability\", when(col(\"empty_taxis\") > 0, col(\"median_fare_tip\")/col(\"empty_taxis\")).otherwise(None))\n",
    ")\n",
    "\n",
    "\n",
    "joined_query = (\n",
    "    df_joined\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")          # Or \"update\", depending on your aggregation logic\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "#######################################\n",
    "# Part 4: foreachBatch to Print Top 10 Results\n",
    "#######################################\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Order by profitability descending and limit to top 10.\n",
    "    rows = batch_df.orderBy(desc(\"profitability\")).limit(10).collect()\n",
    "    if rows:\n",
    "        output = {}\n",
    "        # Use trigger times from the first row as the \"trigger\" event.\n",
    "        trigger_pickup = rows[0][\"trigger_pickup\"]\n",
    "        trigger_dropoff = rows[0][\"trigger_dropoff\"]\n",
    "        output[\"pickup_datetime\"] = str(trigger_pickup) if trigger_pickup else None\n",
    "        output[\"dropoff_datetime\"] = str(trigger_dropoff) if trigger_dropoff else None\n",
    "        for i, row in enumerate(rows, start=1):\n",
    "            cell_id = f\"{row['start_cell_x']}.{row['start_cell_y']}\"\n",
    "            output[f\"profitable_cell_id_{i}\"] = cell_id\n",
    "            output[f\"empty_taxies_in_cell_id_{i}\"] = row[\"empty_taxis\"]\n",
    "            output[f\"median_profit_in_cell_id_{i}\"] = row[\"median_fare_tip\"]\n",
    "            output[f\"profitability_of_cell_{i}\"] = row[\"profitability\"]\n",
    "        # Fill remaining ranks up to 10 with NULL if needed.\n",
    "        for i in range(len(rows)+1, 11):\n",
    "            output[f\"profitable_cell_id_{i}\"] = None\n",
    "            output[f\"empty_taxies_in_cell_id_{i}\"] = None\n",
    "            output[f\"median_profit_in_cell_id_{i}\"] = None\n",
    "            output[f\"profitability_of_cell_{i}\"] = None\n",
    "        # Compute delay as a placeholder.\n",
    "        output[\"delay\"] = 1.0\n",
    "        print(\"\\n=== Top 10 Profitable Areas (Batch {}) ===\".format(batch_id))\n",
    "        print(output)\n",
    "\n",
    "# Write the joined stream using foreachBatch.\n",
    "# IMPORTANT: We use outputMode \"append\" because joins between two streaming DataFrames\n",
    "# require \"append\" mode.\n",
    "query = (df_joined.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60375ba3-a4bf-4ff0-b727-27c23e0aee04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f206f8-ba60-47f6-8f89-02a9a0e68065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0bcd1-dc1b-490e-834b-71f398699cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af646488-ee44-49ce-b02d-28a0bb25e87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e68c7-683b-4c12-a0dc-dde9253cadbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a4439-ca27-4bbd-9bd2-c78e6c0121c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cfc06-85cb-40b0-a6a7-8d2531c12e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5e4e5-20e9-4717-b798-776ea4e8612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46229bbb-462a-4712-a0ea-f244bc62115f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c94d67-a2a4-428e-aab2-eca1743f257f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128768e-a6d0-4378-a352-af74a53f80dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852e8d9-9f8d-45b3-b5c8-ac05e0134a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
